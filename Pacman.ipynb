{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score: 240.0\n",
      "Total Score: 230.0\n",
      "Total Score: 150.0\n",
      "Total Score: 210.0\n",
      "Total Score: 340.0\n",
      "Total Score: 220.0\n",
      "Total Score: 140.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 130.0\n",
      "Total Score: 230.0\n",
      "Total Score: 190.0\n",
      "Total Score: 150.0\n",
      "Total Score: 250.0\n",
      "Total Score: 210.0\n",
      "Total Score: 280.0\n",
      "Total Score: 250.0\n",
      "Total Score: 1010.0\n",
      "Total Score: 110.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 220.0\n",
      "Total Score: 160.0\n",
      "Total Score: 270.0\n",
      "Total Score: 280.0\n",
      "Total Score: 140.0\n",
      "Total Score: 260.0\n",
      "Total Score: 210.0\n",
      "Total Score: 130.0\n",
      "Total Score: 180.0\n",
      "Total Score: 210.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 190.0\n",
      "Total Score: 390.0\n",
      "Total Score: 110.0\n",
      "Total Score: 270.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 330.0\n",
      "Total Score: 190.0\n",
      "Total Score: 320.0\n",
      "Total Score: 280.0\n",
      "Total Score: 190.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 130.0\n",
      "Total Score: 230.0\n",
      "Total Score: 170.0\n",
      "Total Score: 350.0\n",
      "Total Score: 190.0\n",
      "Total Score: 200.0\n",
      "Total Score: 170.0\n",
      "Total Score: 350.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 230.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 190.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 180.0\n",
      "Total Score: 160.0\n",
      "Total Score: 190.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 310.0\n",
      "Total Score: 220.0\n",
      "Total Score: 150.0\n",
      "Total Score: 130.0\n",
      "Total Score: 100.0\n",
      "Total Score: 150.0\n",
      "Total Score: 340.0\n",
      "Total Score: 200.0\n",
      "Total Score: 310.0\n",
      "Total Score: 300.0\n",
      "Total Score: 270.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 130.0\n",
      "Total Score: 200.0\n",
      "Total Score: 490.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 160.0\n",
      "Total Score: 60.0\n",
      "Total Score: 250.0\n",
      "Total Score: 210.0\n",
      "Total Score: 240.0\n",
      "Total Score: 260.0\n",
      "Total Score: 220.0\n",
      "Total Score: 290.0\n",
      "Total Score: 160.0\n",
      "Total Score: 170.0\n",
      "Total Score: 190.0\n",
      "Total Score: 180.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 260.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 250.0\n",
      "Total Score: 200.0\n",
      "Total Score: 120.0\n",
      "Total Score: 290.0\n",
      "Total Score: 160.0\n",
      "Total Score: 160.0\n",
      "Total Score: 140.0\n",
      "Total Score: 170.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 290.0\n",
      "Total Score: 310.0\n",
      "Total Score: 270.0\n",
      "Total Score: 230.0\n",
      "Total Score: 190.0\n",
      "Total Score: 110.0\n",
      "Total Score: 250.0\n",
      "Total Score: 280.0\n",
      "Total Score: 200.0\n",
      "Total Score: 260.0\n",
      "Total Score: 130.0\n",
      "Total Score: 220.0\n",
      "Total Score: 370.0\n",
      "Total Score: 150.0\n",
      "Total Score: 200.0\n",
      "Total Score: 170.0\n",
      "Total Score: 110.0\n",
      "Total Score: 200.0\n",
      "Total Score: 200.0\n",
      "Total Score: 200.0\n",
      "Total Score: 160.0\n",
      "Total Score: 180.0\n",
      "Total Score: 300.0\n",
      "Total Score: 260.0\n",
      "Total Score: 170.0\n",
      "Total Score: 170.0\n",
      "Total Score: 220.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 790.0\n",
      "Total Score: 270.0\n",
      "Total Score: 190.0\n",
      "Total Score: 170.0\n",
      "Total Score: 270.0\n",
      "Total Score: 280.0\n",
      "Total Score: 630.0\n",
      "Total Score: 260.0\n",
      "Total Score: 150.0\n",
      "Total Score: 240.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 230.0\n",
      "Total Score: 100.0\n",
      "Total Score: 230.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 290.0\n",
      "Total Score: 120.0\n",
      "Total Score: 240.0\n",
      "Total Score: 250.0\n",
      "Total Score: 220.0\n",
      "Total Score: 340.0\n",
      "Total Score: 180.0\n",
      "Total Score: 140.0\n",
      "Total Score: 140.0\n",
      "Total Score: 220.0\n",
      "Total Score: 300.0\n",
      "Total Score: 240.0\n",
      "Total Score: 240.0\n",
      "Total Score: 150.0\n",
      "Total Score: 120.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 190.0\n",
      "Total Score: 330.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 280.0\n",
      "Total Score: 250.0\n",
      "Total Score: 240.0\n",
      "Total Score: 210.0\n",
      "Total Score: 280.0\n",
      "Total Score: 280.0\n",
      "Total Score: 390.0\n",
      "Total Score: 110.0\n",
      "Total Score: 190.0\n",
      "Total Score: 110.0\n",
      "Total Score: 170.0\n",
      "Total Score: 150.0\n",
      "Total Score: 130.0\n",
      "Total Score: 140.0\n",
      "Total Score: 160.0\n",
      "Total Score: 180.0\n",
      "Total Score: 190.0\n",
      "Total Score: 650.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 170.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 330.0\n",
      "Total Score: 140.0\n",
      "Total Score: 210.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 230.0\n",
      "Total Score: 180.0\n",
      "Total Score: 330.0\n",
      "Total Score: 290.0\n",
      "Total Score: 190.0\n",
      "Total Score: 280.0\n",
      "Total Score: 220.0\n",
      "Total Score: 300.0\n",
      "Total Score: 570.0\n",
      "Total Score: 610.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 220.0\n",
      "Total Score: 130.0\n",
      "Total Score: 200.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 230.0\n",
      "Total Score: 960.0\n",
      "Total Score: 250.0\n",
      "Total Score: 240.0\n",
      "Total Score: 300.0\n",
      "Total Score: 260.0\n",
      "Total Score: 220.0\n",
      "Total Score: 100.0\n",
      "Total Score: 160.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 180.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 100.0\n",
      "Total Score: 210.0\n",
      "Total Score: 250.0\n",
      "Total Score: 230.0\n",
      "Total Score: 270.0\n",
      "Total Score: 150.0\n",
      "Total Score: 160.0\n",
      "Total Score: 190.0\n",
      "Total Score: 170.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 190.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 190.0\n",
      "Total Score: 290.0\n",
      "Total Score: 300.0\n",
      "Total Score: 280.0\n",
      "Total Score: 190.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 250.0\n",
      "Total Score: 200.0\n",
      "Total Score: 190.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 210.0\n",
      "Total Score: 690.0\n",
      "Total Score: 150.0\n",
      "Total Score: 250.0\n",
      "Total Score: 340.0\n",
      "Total Score: 210.0\n",
      "Total Score: 260.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 90.0\n",
      "Total Score: 150.0\n",
      "Total Score: 170.0\n",
      "Total Score: 110.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 210.0\n",
      "Total Score: 290.0\n",
      "Total Score: 220.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-034a44e5b588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mrandom_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mALL_SCORES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "\n",
    "EPISODES = 500\n",
    "ALL_SCORES = np.zeros(EPISODES)\n",
    "\n",
    "env = gym.make(\"MsPacman-ram-v0\")\n",
    "env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    env.reset()\n",
    "    \n",
    "    reward, info, done = None, None, None\n",
    "\n",
    "    \n",
    "    total_score = 0\n",
    "    while done != True:\n",
    "        env.render()\n",
    "        random_action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(random_action)\n",
    "        total_score += reward\n",
    "    ALL_SCORES[episode] = total_score\n",
    "    print(\"Total Score: {}\".format(total_score))\n",
    "    # print(state, reward, done, info)\n",
    "    \n",
    "env.close()\n",
    "plt.plot(ALL_SCORES)\n",
    "plt.title(\"Random Agent: {} Episodes\".format(EPISODES))\n",
    "plt.show()\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print('Average Score for {} Episodes: {}'.format(EPISODES, np.mean(ALL_SCORES)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2- corrected reward (0.01 LR, batch size 256, min epsilon 0.05, episilon decay 0.9999)  \n",
    "v2- corrected reward (0.05 LR, batch size 256, min epsilon 0.0, episilon decay 0.99999), increased memeory from 2000 to 10000\n",
    "\n",
    "BOTH STILL GET STUCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 25,353\n",
      "Trainable params: 25,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: -488.0   memory length: 748   epsilon: 0.9925478684572383\n",
      "episode: 1   score: -507.0   memory length: 1435   epsilon: 0.9857523997856545\n",
      "episode: 2   score: -324.0   memory length: 1889   epsilon: 0.9812872052258903\n",
      "episode: 3   score: -324.0   memory length: 2483   epsilon: 0.9754756077014011\n",
      "episode: 4   score: -384.0   memory length: 3117   epsilon: 0.9693106251687865\n",
      "episode: 5   score: -384.0   memory length: 3691   epsilon: 0.9637626922407929\n",
      "episode: 6   score: -440.0   memory length: 4401   epsilon: 0.9569441774045132\n",
      "episode: 7   score: -514.0   memory length: 5045   epsilon: 0.9508012277161976\n",
      "episode: 8   score: -360.0   memory length: 5605   epsilon: 0.9454915951399198\n",
      "episode: 9   score: -502.0   memory length: 6377   epsilon: 0.9382204663393856\n",
      "episode: 10   score: -371.0   memory length: 6958   epsilon: 0.9327851830410626\n",
      "episode: 11   score: -398.0   memory length: 7636   epsilon: 0.9264822590424675\n",
      "episode: 12   score: -384.0   memory length: 8220   epsilon: 0.9210873441596589\n",
      "episode: 13   score: -304.0   memory length: 8754   epsilon: 0.9161818226137911\n",
      "episode: 14   score: -349.0   memory length: 9333   epsilon: 0.9108924309795664\n",
      "episode: 15   score: -489.0   memory length: 10000   epsilon: 0.9046379223826599\n",
      "episode: 16   score: -465.0   memory length: 10000   epsilon: 0.8985521485713192\n",
      "episode: 17   score: -288.0   memory length: 10000   epsilon: 0.8939990555390203\n",
      "episode: 18   score: -414.0   memory length: 10000   epsilon: 0.8887932906548333\n",
      "episode: 19   score: -473.0   memory length: 10000   epsilon: 0.882743485832246\n",
      "episode: 20   score: -260.0   memory length: 10000   epsilon: 0.8799232074282763\n",
      "episode: 21   score: -346.0   memory length: 10000   epsilon: 0.8745195139153229\n",
      "episode: 22   score: -489.0   memory length: 10000   epsilon: 0.8683410681413476\n",
      "episode: 23   score: -489.0   memory length: 10000   epsilon: 0.8629826116696993\n",
      "episode: 24   score: -360.0   memory length: 10000   epsilon: 0.8585925824898624\n",
      "episode: 25   score: -426.0   memory length: 10000   epsilon: 0.8534905643050242\n",
      "episode: 26   score: -384.0   memory length: 10000   epsilon: 0.8487752764098196\n",
      "episode: 27   score: -393.0   memory length: 10000   epsilon: 0.8437569080968294\n",
      "episode: 28   score: -335.0   memory length: 10000   epsilon: 0.839087004875908\n",
      "episode: 29   score: -477.0   memory length: 10000   epsilon: 0.8335088903551545\n",
      "episode: 30   score: -311.0   memory length: 10000   epsilon: 0.828597356135029\n",
      "episode: 31   score: -484.0   memory length: 10000   epsilon: 0.8228667699650789\n",
      "episode: 32   score: -400.0   memory length: 10000   epsilon: 0.8176172128907016\n",
      "episode: 33   score: -486.0   memory length: 10000   epsilon: 0.8112970257594472\n",
      "episode: 34   score: -315.0   memory length: 10000   epsilon: 0.8053558235787556\n",
      "episode: 35   score: -441.0   memory length: 10000   epsilon: 0.800610184110893\n",
      "episode: 36   score: -472.0   memory length: 10000   epsilon: 0.7950095538277607\n",
      "episode: 37   score: -414.0   memory length: 10000   epsilon: 0.789827130429894\n",
      "episode: 38   score: -455.0   memory length: 10000   epsilon: 0.7842784036607982\n",
      "episode: 39   score: -410.0   memory length: 10000   epsilon: 0.7778735843742112\n",
      "episode: 40   score: -515.0   memory length: 10000   epsilon: 0.7716368075428628\n",
      "episode: 41   score: -169.0   memory length: 10000   epsilon: 0.7649603017359553\n",
      "episode: 42   score: -455.0   memory length: 10000   epsilon: 0.7602702092468528\n",
      "episode: 43   score: -392.0   memory length: 10000   epsilon: 0.7562362912999665\n",
      "episode: 44   score: -410.0   memory length: 10000   epsilon: 0.751186418862353\n",
      "episode: 45   score: -544.0   memory length: 10000   epsilon: 0.7450965503544277\n",
      "episode: 46   score: -385.0   memory length: 10000   epsilon: 0.7406023025615533\n",
      "episode: 47   score: -457.0   memory length: 10000   epsilon: 0.7354582267220077\n",
      "episode: 48   score: -304.0   memory length: 10000   epsilon: 0.7317608240537239\n",
      "episode: 49   score: -415.0   memory length: 10000   epsilon: 0.7272742828084793\n",
      "episode: 50   score: -422.0   memory length: 10000   epsilon: 0.7223311228194018\n",
      "episode: 51   score: -490.0   memory length: 10000   epsilon: 0.7176511734342051\n",
      "episode: 52   score: -351.0   memory length: 10000   epsilon: 0.713779144507966\n",
      "episode: 53   score: -294.0   memory length: 10000   epsilon: 0.7101197142006787\n",
      "episode: 54   score: -381.0   memory length: 10000   epsilon: 0.7045952527135475\n",
      "episode: 55   score: -443.0   memory length: 10000   epsilon: 0.6997292634093534\n",
      "episode: 56   score: -480.0   memory length: 10000   epsilon: 0.6941537332811971\n",
      "episode: 57   score: -461.0   memory length: 10000   epsilon: 0.6895115305277427\n",
      "episode: 58   score: -292.0   memory length: 10000   epsilon: 0.6854416606188266\n",
      "episode: 59   score: -457.0   memory length: 10000   epsilon: 0.6801363900569003\n",
      "episode: 60   score: -399.0   memory length: 10000   epsilon: 0.6756014412503722\n",
      "episode: 61   score: -477.0   memory length: 10000   epsilon: 0.6710430442852866\n",
      "episode: 62   score: -390.0   memory length: 10000   epsilon: 0.6671622404347982\n",
      "episode: 63   score: -303.0   memory length: 10000   epsilon: 0.6634829973323982\n",
      "episode: 64   score: -372.0   memory length: 10000   epsilon: 0.6591052244173666\n",
      "episode: 65   score: -236.0   memory length: 10000   epsilon: 0.6534219886866968\n",
      "episode: 66   score: -486.0   memory length: 10000   epsilon: 0.6486304334452657\n",
      "episode: 67   score: -411.0   memory length: 10000   epsilon: 0.6443571038272121\n",
      "episode: 68   score: -254.0   memory length: 10000   epsilon: 0.6410535891019591\n",
      "episode: 69   score: -421.0   memory length: 10000   epsilon: 0.635812058851271\n",
      "episode: 70   score: -461.0   memory length: 10000   epsilon: 0.6301090941230211\n",
      "episode: 71   score: -295.0   memory length: 10000   epsilon: 0.6263710580082701\n",
      "episode: 72   score: -280.0   memory length: 10000   epsilon: 0.6208210619591475\n",
      "episode: 73   score: -443.0   memory length: 10000   epsilon: 0.6167802877071827\n",
      "episode: 74   score: -289.0   memory length: 10000   epsilon: 0.6115659631216448\n",
      "episode: 75   score: -482.0   memory length: 10000   epsilon: 0.6063168942634818\n",
      "episode: 76   score: -427.0   memory length: 10000   epsilon: 0.6020453302349272\n",
      "episode: 77   score: -588.0   memory length: 10000   epsilon: 0.5969018526567264\n",
      "episode: 78   score: -347.0   memory length: 10000   epsilon: 0.5913349760097075\n",
      "episode: 79   score: -343.0   memory length: 10000   epsilon: 0.587603629127002\n",
      "episode: 80   score: -339.0   memory length: 10000   epsilon: 0.5838024108634915\n",
      "episode: 81   score: -374.0   memory length: 10000   epsilon: 0.5802288283182108\n",
      "episode: 82   score: -604.0   memory length: 10000   epsilon: 0.5729981537404599\n",
      "episode: 83   score: -368.0   memory length: 10000   epsilon: 0.569410982904934\n",
      "episode: 84   score: -409.0   memory length: 10000   epsilon: 0.5652750499522659\n",
      "episode: 85   score: -554.0   memory length: 10000   epsilon: 0.5602439882048794\n",
      "episode: 86   score: -301.0   memory length: 10000   epsilon: 0.5554965172678555\n",
      "episode: 87   score: -307.0   memory length: 10000   epsilon: 0.5524662315541611\n",
      "episode: 88   score: -319.0   memory length: 10000   epsilon: 0.5486728036177355\n",
      "episode: 89   score: 873.0   memory length: 10000   epsilon: 0.5439363489287744\n",
      "episode: 90   score: -381.0   memory length: 10000   epsilon: 0.5398126831443176\n",
      "episode: 91   score: -337.0   memory length: 10000   epsilon: 0.5366532489086251\n",
      "episode: 92   score: -353.0   memory length: 10000   epsilon: 0.533320275495471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 93   score: -397.0   memory length: 10000   epsilon: 0.5300928105516604\n",
      "episode: 94   score: -545.0   memory length: 10000   epsilon: 0.5245297005889874\n",
      "episode: 95   score: -386.0   memory length: 10000   epsilon: 0.5209437228795372\n",
      "episode: 96   score: -277.0   memory length: 10000   epsilon: 0.5162917298063356\n",
      "episode: 97   score: -400.0   memory length: 10000   epsilon: 0.5125365046508248\n",
      "episode: 98   score: -315.0   memory length: 10000   epsilon: 0.5092412662875729\n",
      "episode: 99   score: -254.0   memory length: 10000   epsilon: 0.5066811344563399\n",
      "episode: 100   score: -379.0   memory length: 10000   epsilon: 0.5033530671660001\n",
      "episode: 101   score: -394.0   memory length: 10000   epsilon: 0.5000218579977569\n",
      "episode: 102   score: -333.0   memory length: 10000   epsilon: 0.49696608424481997\n",
      "episode: 103   score: -378.0   memory length: 10000   epsilon: 0.4942501449937592\n",
      "episode: 104   score: -341.0   memory length: 10000   epsilon: 0.49143600461199966\n",
      "episode: 105   score: -307.0   memory length: 10000   epsilon: 0.48836432528246776\n",
      "episode: 106   score: -381.0   memory length: 10000   epsilon: 0.4840807111852952\n",
      "episode: 107   score: -143.0   memory length: 10000   epsilon: 0.4791058718481566\n",
      "episode: 108   score: -284.0   memory length: 10000   epsilon: 0.4768879590501495\n",
      "episode: 109   score: -298.0   memory length: 10000   epsilon: 0.4746613266525033\n",
      "episode: 110   score: -320.0   memory length: 10000   epsilon: 0.47215226396536736\n",
      "episode: 111   score: -328.0   memory length: 10000   epsilon: 0.46914950648931625\n",
      "episode: 112   score: -298.0   memory length: 10000   epsilon: 0.46639898808748737\n",
      "episode: 113   score: -171.0   memory length: 10000   epsilon: 0.46402175637212517\n",
      "episode: 114   score: -319.0   memory length: 10000   epsilon: 0.4610660959627967\n",
      "episode: 115   score: -263.0   memory length: 10000   epsilon: 0.45687569982135307\n",
      "episode: 116   score: -559.0   memory length: 10000   epsilon: 0.45206289089851787\n",
      "episode: 117   score: -248.0   memory length: 10000   epsilon: 0.4486491913831328\n",
      "episode: 118   score: 202.0   memory length: 10000   epsilon: 0.44548395743201596\n",
      "episode: 119   score: -224.0   memory length: 10000   epsilon: 0.4423587483890523\n",
      "episode: 120   score: -435.0   memory length: 10000   epsilon: 0.4384172822547794\n",
      "episode: 121   score: -434.0   memory length: 10000   epsilon: 0.43599515389094845\n",
      "episode: 122   score: -271.0   memory length: 10000   epsilon: 0.4333826683363556\n",
      "episode: 123   score: -210.0   memory length: 10000   epsilon: 0.43040260606280284\n",
      "episode: 124   score: -288.0   memory length: 10000   epsilon: 0.42822169412590055\n",
      "episode: 125   score: -259.0   memory length: 10000   epsilon: 0.4261754067503942\n",
      "episode: 126   score: 1091.0   memory length: 10000   epsilon: 0.4223190069122962\n",
      "episode: 127   score: -221.0   memory length: 10000   epsilon: 0.4192011733216016\n",
      "episode: 128   score: -427.0   memory length: 10000   epsilon: 0.414917987660802\n",
      "episode: 129   score: -130.0   memory length: 10000   epsilon: 0.41070731407522365\n",
      "episode: 130   score: -208.0   memory length: 10000   epsilon: 0.4080137181299037\n",
      "episode: 131   score: 76.0   memory length: 10000   epsilon: 0.4043418774843254\n",
      "episode: 132   score: -195.0   memory length: 10000   epsilon: 0.40130057646103756\n",
      "episode: 133   score: -276.0   memory length: 10000   epsilon: 0.3979198772021149\n",
      "episode: 134   score: -111.0   memory length: 10000   epsilon: 0.3954959871578114\n",
      "episode: 135   score: -264.0   memory length: 10000   epsilon: 0.39331098650421187\n",
      "episode: 136   score: -347.0   memory length: 10000   epsilon: 0.39132193636020646\n",
      "episode: 137   score: -228.0   memory length: 10000   epsilon: 0.3888721228167428\n",
      "episode: 138   score: -181.0   memory length: 10000   epsilon: 0.3852685061996206\n",
      "episode: 139   score: -292.0   memory length: 10000   epsilon: 0.3825733787758352\n",
      "episode: 140   score: -164.0   memory length: 10000   epsilon: 0.38087850989298305\n",
      "episode: 141   score: -396.0   memory length: 10000   epsilon: 0.3785394981295055\n",
      "episode: 142   score: -296.0   memory length: 10000   epsilon: 0.37644064823529516\n",
      "episode: 143   score: -213.0   memory length: 10000   epsilon: 0.37470174799990313\n",
      "episode: 144   score: -458.0   memory length: 10000   epsilon: 0.3719838109794322\n",
      "episode: 145   score: -246.0   memory length: 10000   epsilon: 0.37014333024004514\n",
      "episode: 146   score: 47.0   memory length: 10000   epsilon: 0.3650959665221796\n",
      "episode: 147   score: -360.0   memory length: 10000   epsilon: 0.3628756488784763\n",
      "episode: 148   score: -365.0   memory length: 10000   epsilon: 0.3605786776252972\n",
      "episode: 149   score: -268.0   memory length: 10000   epsilon: 0.35814221096404897\n",
      "episode: 150   score: -315.0   memory length: 10000   epsilon: 0.3553062540724229\n",
      "episode: 151   score: -148.0   memory length: 10000   epsilon: 0.3534351716998839\n",
      "episode: 152   score: -263.0   memory length: 10000   epsilon: 0.35155636431029663\n",
      "episode: 153   score: -159.0   memory length: 10000   epsilon: 0.34984144151772634\n",
      "episode: 154   score: -547.0   memory length: 10000   epsilon: 0.3468214118659126\n",
      "episode: 155   score: -259.0   memory length: 10000   epsilon: 0.3440613361707998\n",
      "episode: 156   score: -553.0   memory length: 10000   epsilon: 0.34124131756795945\n",
      "episode: 157   score: -292.0   memory length: 10000   epsilon: 0.3396004837992131\n",
      "episode: 158   score: -287.0   memory length: 10000   epsilon: 0.33768038807107853\n",
      "episode: 159   score: -852.0   memory length: 10000   epsilon: 0.33331226576128953\n",
      "episode: 160   score: -600.0   memory length: 10000   epsilon: 0.3306233278642329\n",
      "episode: 161   score: -340.0   memory length: 10000   epsilon: 0.3280872918284087\n",
      "episode: 162   score: -239.0   memory length: 10000   epsilon: 0.327075061736407\n",
      "episode: 163   score: -263.0   memory length: 10000   epsilon: 0.32536891609266944\n",
      "episode: 164   score: -460.0   memory length: 10000   epsilon: 0.32280861535767674\n",
      "episode: 165   score: -434.0   memory length: 10000   epsilon: 0.32041581953342635\n",
      "episode: 166   score: -403.0   memory length: 10000   epsilon: 0.3183939831498371\n",
      "episode: 167   score: -843.0   memory length: 10000   epsilon: 0.3149643640729734\n",
      "episode: 168   score: -502.0   memory length: 10000   epsilon: 0.31257344463617326\n",
      "episode: 169   score: -243.0   memory length: 10000   epsilon: 0.310849675483079\n",
      "episode: 170   score: -482.0   memory length: 10000   epsilon: 0.30876775833057496\n",
      "episode: 171   score: -285.0   memory length: 10000   epsilon: 0.307028130873222\n",
      "episode: 172   score: -371.0   memory length: 10000   epsilon: 0.3051884164874043\n",
      "episode: 173   score: -475.0   memory length: 10000   epsilon: 0.30289290891066384\n",
      "episode: 174   score: -1069.0   memory length: 10000   epsilon: 0.2988641852151727\n",
      "episode: 175   score: -222.0   memory length: 10000   epsilon: 0.29710012718219275\n",
      "episode: 176   score: -201.0   memory length: 10000   epsilon: 0.29573364093431753\n",
      "episode: 177   score: -482.0   memory length: 10000   epsilon: 0.29366485038177453\n",
      "episode: 178   score: -360.0   memory length: 10000   epsilon: 0.29173303471439277\n",
      "episode: 179   score: -294.0   memory length: 10000   epsilon: 0.2901793246384623\n",
      "episode: 180   score: -555.0   memory length: 10000   epsilon: 0.28782396250400805\n",
      "episode: 181   score: -357.0   memory length: 10000   epsilon: 0.2850541047833116\n",
      "episode: 182   score: -498.0   memory length: 10000   epsilon: 0.28261879082455127\n",
      "episode: 183   score: -253.0   memory length: 10000   epsilon: 0.2812570265641153\n",
      "episode: 184   score: -113.0   memory length: 10000   epsilon: 0.27886809545914115\n",
      "episode: 185   score: -111.0   memory length: 10000   epsilon: 0.2767539531961974\n",
      "episode: 186   score: -95.0   memory length: 10000   epsilon: 0.2741235201009367\n",
      "episode: 187   score: -135.0   memory length: 10000   epsilon: 0.272415582425563\n",
      "episode: 188   score: 560.0   memory length: 10000   epsilon: 0.2695971276446668\n",
      "episode: 189   score: -149.0   memory length: 10000   epsilon: 0.2676389012007703\n",
      "episode: 190   score: -100.0   memory length: 10000   epsilon: 0.26516138533753797\n",
      "episode: 191   score: -115.0   memory length: 10000   epsilon: 0.26337756423658765\n",
      "episode: 192   score: -154.0   memory length: 10000   epsilon: 0.2611378856790424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 193   score: -85.0   memory length: 10000   epsilon: 0.2595887208491703\n",
      "episode: 194   score: -285.0   memory length: 10000   epsilon: 0.2575589166464798\n",
      "episode: 195   score: -298.0   memory length: 10000   epsilon: 0.2558697340344737\n",
      "episode: 196   score: -201.0   memory length: 10000   epsilon: 0.254947704948297\n",
      "episode: 197   score: -341.0   memory length: 10000   epsilon: 0.25235791928521095\n",
      "episode: 198   score: -223.0   memory length: 10000   epsilon: 0.2511670789304479\n",
      "episode: 199   score: -186.0   memory length: 10000   epsilon: 0.24957471742378298\n",
      "episode: 200   score: -297.0   memory length: 10000   epsilon: 0.2465064790071109\n",
      "episode: 201   score: -284.0   memory length: 10000   epsilon: 0.24497306015348885\n",
      "episode: 202   score: -157.0   memory length: 10000   epsilon: 0.24273691440902537\n",
      "episode: 203   score: -154.0   memory length: 10000   epsilon: 0.2411545869279244\n",
      "episode: 204   score: -200.0   memory length: 10000   epsilon: 0.23932874990715658\n",
      "episode: 205   score: -319.0   memory length: 10000   epsilon: 0.23794703346578805\n",
      "episode: 206   score: -194.0   memory length: 10000   epsilon: 0.23608881317394156\n",
      "episode: 207   score: -136.0   memory length: 10000   epsilon: 0.2352027815894933\n",
      "episode: 208   score: -176.0   memory length: 10000   epsilon: 0.23392206752951555\n",
      "episode: 209   score: -250.0   memory length: 10000   epsilon: 0.23247623020885375\n",
      "episode: 210   score: -439.0   memory length: 10000   epsilon: 0.23076455473996463\n",
      "episode: 211   score: 216.0   memory length: 10000   epsilon: 0.22887085808546337\n",
      "episode: 212   score: 241.0   memory length: 10000   epsilon: 0.22695865493436457\n",
      "episode: 213   score: -368.0   memory length: 10000   epsilon: 0.22515472310926718\n",
      "episode: 214   score: -107.0   memory length: 10000   epsilon: 0.223501424422648\n",
      "episode: 215   score: -289.0   memory length: 10000   epsilon: 0.22258916587589328\n",
      "episode: 216   score: -237.0   memory length: 10000   epsilon: 0.22137492105453163\n",
      "episode: 217   score: -15.0   memory length: 10000   epsilon: 0.21962195892446676\n",
      "episode: 218   score: -510.0   memory length: 10000   epsilon: 0.2181117759798028\n",
      "episode: 219   score: -354.0   memory length: 10000   epsilon: 0.21684170906285452\n",
      "episode: 220   score: -335.0   memory length: 10000   epsilon: 0.21516767271720277\n",
      "episode: 221   score: -228.0   memory length: 10000   epsilon: 0.21414162033020095\n",
      "episode: 222   score: -496.0   memory length: 10000   epsilon: 0.2122739228561469\n",
      "episode: 223   score: -546.0   memory length: 10000   epsilon: 0.21023321902650627\n",
      "episode: 224   score: -295.0   memory length: 10000   epsilon: 0.20865192720451511\n",
      "episode: 225   score: -204.0   memory length: 10000   epsilon: 0.20729178929881498\n",
      "episode: 226   score: -544.0   memory length: 10000   epsilon: 0.20548794432629996\n",
      "episode: 227   score: 1139.0   memory length: 10000   epsilon: 0.20393010836478453\n",
      "episode: 228   score: -362.0   memory length: 10000   epsilon: 0.2024427827877661\n",
      "episode: 229   score: -318.0   memory length: 10000   epsilon: 0.2013364249953565\n",
      "episode: 230   score: -505.0   memory length: 10000   epsilon: 0.1997820907549453\n",
      "episode: 231   score: -287.0   memory length: 10000   epsilon: 0.19843412726556794\n",
      "episode: 232   score: -232.0   memory length: 10000   epsilon: 0.19757873653311567\n",
      "episode: 233   score: -237.0   memory length: 10000   epsilon: 0.19661886247726307\n",
      "episode: 234   score: -307.0   memory length: 10000   epsilon: 0.19513606972587436\n",
      "episode: 235   score: -296.0   memory length: 10000   epsilon: 0.19397651186739362\n",
      "episode: 236   score: 149.0   memory length: 10000   epsilon: 0.19179498639152315\n",
      "episode: 237   score: -278.0   memory length: 10000   epsilon: 0.19036570492369084\n",
      "episode: 238   score: -330.0   memory length: 10000   epsilon: 0.18932156235009223\n",
      "episode: 239   score: -3.0   memory length: 10000   epsilon: 0.18767597034323666\n",
      "episode: 240   score: -384.0   memory length: 10000   epsilon: 0.18637800177971406\n",
      "episode: 241   score: -295.0   memory length: 10000   epsilon: 0.18464348031470354\n",
      "episode: 242   score: -115.0   memory length: 10000   epsilon: 0.18345635652091313\n",
      "episode: 243   score: -438.0   memory length: 10000   epsilon: 0.18230785492905296\n",
      "episode: 244   score: -582.0   memory length: 10000   epsilon: 0.18065275773167913\n",
      "episode: 245   score: -229.0   memory length: 10000   epsilon: 0.17905386433249496\n",
      "episode: 246   score: -323.0   memory length: 10000   epsilon: 0.17785287169385403\n",
      "episode: 247   score: -320.0   memory length: 10000   epsilon: 0.17670057129969324\n",
      "episode: 248   score: 40.0   memory length: 10000   epsilon: 0.17538026774071744\n",
      "episode: 249   score: 110.0   memory length: 10000   epsilon: 0.17377416066794543\n",
      "episode: 250   score: -241.0   memory length: 10000   epsilon: 0.17278473276833742\n",
      "episode: 251   score: -474.0   memory length: 10000   epsilon: 0.17076809077340252\n",
      "episode: 252   score: -465.0   memory length: 10000   epsilon: 0.16939891910661328\n",
      "episode: 253   score: -427.0   memory length: 10000   epsilon: 0.16822230801047253\n",
      "episode: 254   score: -740.0   memory length: 10000   epsilon: 0.16601635384298227\n",
      "episode: 255   score: -220.0   memory length: 10000   epsilon: 0.16494074169149778\n",
      "episode: 256   score: -195.0   memory length: 10000   epsilon: 0.16396225327578146\n",
      "episode: 257   score: -253.0   memory length: 10000   epsilon: 0.16273224813981166\n",
      "episode: 258   score: -473.0   memory length: 10000   epsilon: 0.16122100959893834\n",
      "episode: 259   score: -154.0   memory length: 10000   epsilon: 0.1599459769856103\n",
      "episode: 260   score: -586.0   memory length: 10000   epsilon: 0.15844001494491475\n",
      "episode: 261   score: -382.0   memory length: 10000   epsilon: 0.15722156122015982\n",
      "episode: 262   score: -502.0   memory length: 10000   epsilon: 0.1554829334524351\n",
      "episode: 263   score: -143.0   memory length: 10000   epsilon: 0.1546564099264371\n",
      "episode: 264   score: -146.0   memory length: 10000   epsilon: 0.15306243260900262\n",
      "episode: 265   score: -245.0   memory length: 10000   epsilon: 0.15198713197112954\n",
      "episode: 266   score: -358.0   memory length: 10000   epsilon: 0.1512169913035263\n",
      "episode: 267   score: -204.0   memory length: 10000   epsilon: 0.1503514878172945\n",
      "episode: 268   score: -123.0   memory length: 10000   epsilon: 0.14920866566197513\n",
      "episode: 269   score: -118.0   memory length: 10000   epsilon: 0.14845260388296294\n",
      "episode: 270   score: -171.0   memory length: 10000   epsilon: 0.1475040612762429\n",
      "episode: 271   score: -531.0   memory length: 10000   epsilon: 0.14606411372606412\n",
      "episode: 272   score: -237.0   memory length: 10000   epsilon: 0.14506408728863818\n",
      "episode: 273   score: -269.0   memory length: 10000   epsilon: 0.14342117327674958\n",
      "episode: 274   score: 400.0   memory length: 10000   epsilon: 0.14139897409912033\n",
      "episode: 275   score: -311.0   memory length: 10000   epsilon: 0.14021478928279246\n",
      "episode: 276   score: -565.0   memory length: 10000   epsilon: 0.13884044648679345\n",
      "episode: 277   score: 74.0   memory length: 10000   epsilon: 0.1375332023405962\n",
      "episode: 278   score: 744.0   memory length: 10000   epsilon: 0.13626551699011055\n",
      "episode: 279   score: -124.0   memory length: 10000   epsilon: 0.1349312330744747\n",
      "episode: 280   score: -219.0   memory length: 10000   epsilon: 0.13377713197269137\n",
      "episode: 281   score: 374.0   memory length: 10000   epsilon: 0.13278286283179583\n",
      "episode: 282   score: -321.0   memory length: 10000   epsilon: 0.1315655409506472\n",
      "episode: 283   score: -205.0   memory length: 10000   epsilon: 0.13073274253497363\n",
      "episode: 284   score: -367.0   memory length: 10000   epsilon: 0.1295523518221959\n",
      "episode: 285   score: -257.0   memory length: 10000   epsilon: 0.1284468265813975\n",
      "episode: 286   score: 315.0   memory length: 10000   epsilon: 0.12714967896631904\n",
      "episode: 287   score: -364.0   memory length: 10000   epsilon: 0.1260432274761941\n",
      "episode: 288   score: -282.0   memory length: 10000   epsilon: 0.12502389546117967\n",
      "episode: 289   score: -159.0   memory length: 10000   epsilon: 0.12447624080334578\n",
      "episode: 290   score: -65.0   memory length: 10000   epsilon: 0.12355233401588533\n",
      "episode: 291   score: -143.0   memory length: 10000   epsilon: 0.12290783891027969\n",
      "episode: 292   score: 155.0   memory length: 10000   epsilon: 0.12192239760271387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 293   score: -186.0   memory length: 10000   epsilon: 0.12099203522232775\n",
      "episode: 294   score: -302.0   memory length: 10000   epsilon: 0.12007357509887093\n",
      "episode: 295   score: -204.0   memory length: 10000   epsilon: 0.11927892374910348\n",
      "episode: 296   score: -158.0   memory length: 10000   epsilon: 0.11876949835632727\n",
      "episode: 297   score: -314.0   memory length: 10000   epsilon: 0.11746548795643603\n",
      "episode: 298   score: -71.0   memory length: 10000   epsilon: 0.11672661064257853\n",
      "episode: 299   score: -214.0   memory length: 10000   epsilon: 0.11564145424249615\n",
      "episode: 300   score: -355.0   memory length: 10000   epsilon: 0.11497842061391718\n",
      "episode: 301   score: -408.0   memory length: 10000   epsilon: 0.1139505313874562\n",
      "episode: 302   score: -278.0   memory length: 10000   epsilon: 0.11304482021237\n",
      "episode: 303   score: -318.0   memory length: 10000   epsilon: 0.11232588650193771\n",
      "episode: 304   score: -179.0   memory length: 10000   epsilon: 0.11176677367938816\n",
      "episode: 305   score: -532.0   memory length: 10000   epsilon: 0.11057502708140907\n",
      "episode: 306   score: -166.0   memory length: 10000   epsilon: 0.10908574243769546\n",
      "episode: 307   score: 10.0   memory length: 10000   epsilon: 0.10780608908174341\n",
      "episode: 308   score: -575.0   memory length: 10000   epsilon: 0.10699521396022268\n",
      "episode: 309   score: -384.0   memory length: 10000   epsilon: 0.10591575917825556\n",
      "episode: 310   score: 11.0   memory length: 10000   epsilon: 0.10498883482118967\n",
      "episode: 311   score: -256.0   memory length: 10000   epsilon: 0.10447982443603068\n",
      "episode: 312   score: -223.0   memory length: 10000   epsilon: 0.10398679932491307\n",
      "episode: 313   score: -629.0   memory length: 10000   epsilon: 0.10292225630419405\n",
      "episode: 314   score: 492.0   memory length: 10000   epsilon: 0.10206336727322289\n",
      "episode: 315   score: -307.0   memory length: 10000   epsilon: 0.101395005580744\n",
      "episode: 316   score: 28.0   memory length: 10000   epsilon: 0.10042425755777122\n",
      "episode: 317   score: -262.0   memory length: 10000   epsilon: 0.09995136959119437\n",
      "episode: 318   score: -177.0   memory length: 10000   epsilon: 0.09934649939168476\n",
      "episode: 319   score: -238.0   memory length: 10000   epsilon: 0.09859629603799984\n",
      "episode: 320   score: -331.0   memory length: 10000   epsilon: 0.09767285189388236\n",
      "episode: 321   score: -176.0   memory length: 10000   epsilon: 0.09709245010997401\n",
      "episode: 322   score: -306.0   memory length: 10000   epsilon: 0.09619751947102632\n",
      "episode: 323   score: -288.0   memory length: 10000   epsilon: 0.09530893147270791\n",
      "episode: 324   score: -97.0   memory length: 10000   epsilon: 0.09447672255274513\n",
      "episode: 325   score: -368.0   memory length: 10000   epsilon: 0.09370705136080881\n",
      "episode: 326   score: -167.0   memory length: 10000   epsilon: 0.09320519068761787\n",
      "episode: 327   score: -318.0   memory length: 10000   epsilon: 0.09266801593013564\n",
      "episode: 328   score: -167.0   memory length: 10000   epsilon: 0.0922454871982181\n",
      "episode: 329   score: -190.0   memory length: 10000   epsilon: 0.09143728417876401\n",
      "episode: 330   score: -307.0   memory length: 10000   epsilon: 0.09086576314016576\n",
      "episode: 331   score: -273.0   memory length: 10000   epsilon: 0.09042793751512306\n",
      "episode: 332   score: -165.0   memory length: 10000   epsilon: 0.08971188192396207\n",
      "episode: 333   score: -526.0   memory length: 10000   epsilon: 0.08876950425042422\n",
      "episode: 334   score: -193.0   memory length: 10000   epsilon: 0.08841248124772919\n",
      "episode: 335   score: -335.0   memory length: 10000   epsilon: 0.08785283794161076\n",
      "episode: 336   score: -398.0   memory length: 10000   epsilon: 0.08727666108779189\n",
      "episode: 337   score: -188.0   memory length: 10000   epsilon: 0.08673027836171748\n",
      "episode: 338   score: -372.0   memory length: 10000   epsilon: 0.08606329514566811\n",
      "episode: 339   score: -257.0   memory length: 10000   epsilon: 0.08515839242736781\n",
      "episode: 340   score: -229.0   memory length: 10000   epsilon: 0.08461596249695366\n",
      "episode: 341   score: -231.0   memory length: 10000   epsilon: 0.08426890285191167\n",
      "episode: 342   score: -239.0   memory length: 10000   epsilon: 0.08374888685311548\n",
      "episode: 343   score: -351.0   memory length: 10000   epsilon: 0.083064119882983\n",
      "episode: 344   score: -278.0   memory length: 10000   epsilon: 0.08239566265601483\n",
      "episode: 345   score: -204.0   memory length: 10000   epsilon: 0.08198143118522515\n",
      "episode: 346   score: 85.0   memory length: 10000   epsilon: 0.08129160327014431\n",
      "episode: 347   score: -251.0   memory length: 10000   epsilon: 0.08062692810895512\n",
      "episode: 348   score: -196.0   memory length: 10000   epsilon: 0.0802520788268896\n",
      "episode: 349   score: -221.0   memory length: 10000   epsilon: 0.07984303465680402\n",
      "episode: 350   score: -201.0   memory length: 10000   epsilon: 0.07946785632623184\n",
      "episode: 351   score: -174.0   memory length: 10000   epsilon: 0.07892614823411348\n",
      "episode: 352   score: -110.0   memory length: 10000   epsilon: 0.07841478943032493\n",
      "episode: 353   score: -281.0   memory length: 10000   epsilon: 0.07782809721497815\n",
      "episode: 354   score: -232.0   memory length: 10000   epsilon: 0.07681365678821674\n",
      "episode: 355   score: -135.0   memory length: 10000   epsilon: 0.07637324346136687\n",
      "episode: 356   score: -140.0   memory length: 10000   epsilon: 0.07593155856525895\n",
      "episode: 357   score: -288.0   memory length: 10000   epsilon: 0.07534310016870527\n",
      "episode: 358   score: 1210.0   memory length: 10000   epsilon: 0.07464565400144177\n",
      "episode: 359   score: -410.0   memory length: 10000   epsilon: 0.07414719732187455\n",
      "episode: 360   score: -372.0   memory length: 10000   epsilon: 0.07357698197073108\n",
      "episode: 361   score: -367.0   memory length: 10000   epsilon: 0.07312440735753632\n",
      "episode: 362   score: -437.0   memory length: 10000   epsilon: 0.07242794120284196\n",
      "episode: 363   score: -200.0   memory length: 10000   epsilon: 0.07208832689990415\n",
      "episode: 364   score: 35.0   memory length: 10000   epsilon: 0.07133178902278821\n",
      "episode: 365   score: -319.0   memory length: 10000   epsilon: 0.07088451797558418\n",
      "episode: 366   score: -223.0   memory length: 10000   epsilon: 0.0702543340382652\n",
      "episode: 367   score: -267.0   memory length: 10000   epsilon: 0.06992700955922643\n",
      "episode: 368   score: -123.0   memory length: 10000   epsilon: 0.06940243405979009\n",
      "episode: 369   score: -190.0   memory length: 10000   epsilon: 0.06884942680069356\n",
      "episode: 370   score: -339.0   memory length: 10000   epsilon: 0.06844509358542465\n",
      "episode: 371   score: -237.0   memory length: 10000   epsilon: 0.06805130061174673\n",
      "episode: 372   score: -194.0   memory length: 10000   epsilon: 0.0674388860764808\n",
      "episode: 373   score: -200.0   memory length: 10000   epsilon: 0.0667745317625358\n",
      "episode: 374   score: -80.0   memory length: 10000   epsilon: 0.06614317425053497\n",
      "episode: 375   score: -298.0   memory length: 10000   epsilon: 0.06547324909161001\n",
      "episode: 376   score: -434.0   memory length: 10000   epsilon: 0.06488403523303059\n",
      "episode: 377   score: -407.0   memory length: 10000   epsilon: 0.06413123590553699\n",
      "episode: 378   score: -277.0   memory length: 10000   epsilon: 0.06358397683212032\n",
      "episode: 379   score: -210.0   memory length: 10000   epsilon: 0.06315307037750685\n",
      "episode: 380   score: -156.0   memory length: 10000   epsilon: 0.06285946024443509\n",
      "episode: 381   score: -201.0   memory length: 10000   epsilon: 0.06240163070658676\n",
      "episode: 382   score: -195.0   memory length: 10000   epsilon: 0.06213077180394346\n",
      "episode: 383   score: -265.0   memory length: 10000   epsilon: 0.06163262433386012\n",
      "episode: 384   score: -130.0   memory length: 10000   epsilon: 0.06132522880444584\n",
      "episode: 385   score: 425.0   memory length: 10000   epsilon: 0.060748432164844136\n",
      "episode: 386   score: -150.0   memory length: 10000   epsilon: 0.060137958031535356\n",
      "episode: 387   score: -286.0   memory length: 10000   epsilon: 0.059541954016575846\n",
      "episode: 388   score: -235.0   memory length: 10000   epsilon: 0.05918872950905366\n",
      "episode: 389   score: -189.0   memory length: 10000   epsilon: 0.05892356652412129\n",
      "episode: 390   score: -168.0   memory length: 10000   epsilon: 0.0584727645744818\n",
      "episode: 391   score: -301.0   memory length: 10000   epsilon: 0.05812239542146567\n",
      "episode: 392   score: 587.0   memory length: 10000   epsilon: 0.05750782352199752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 393   score: -8.0   memory length: 10000   epsilon: 0.05713637575886294\n",
      "episode: 394   score: -285.0   memory length: 10000   epsilon: 0.05648024379443767\n",
      "episode: 395   score: -294.0   memory length: 10000   epsilon: 0.05594957633092786\n",
      "episode: 396   score: -217.0   memory length: 10000   epsilon: 0.05567776293795685\n",
      "episode: 397   score: -75.0   memory length: 10000   epsilon: 0.05528108476396874\n",
      "episode: 398   score: -301.0   memory length: 10000   epsilon: 0.05485101893473145\n",
      "episode: 399   score: 147.0   memory length: 10000   epsilon: 0.05455398359751995\n",
      "episode: 400   score: -180.0   memory length: 10000   epsilon: 0.0542873216000236\n",
      "episode: 401   score: -34.0   memory length: 10000   epsilon: 0.05393883284469524\n",
      "episode: 402   score: -319.0   memory length: 10000   epsilon: 0.05365961410552291\n",
      "episode: 403   score: -266.0   memory length: 10000   epsilon: 0.05330342671523259\n",
      "episode: 404   score: 32.0   memory length: 10000   epsilon: 0.052821619904823425\n",
      "episode: 405   score: 359.0   memory length: 10000   epsilon: 0.052426413421132016\n",
      "episode: 406   score: -307.0   memory length: 10000   epsilon: 0.05192189077916833\n",
      "episode: 407   score: -335.0   memory length: 10000   epsilon: 0.05157775400778762\n",
      "episode: 408   score: -257.0   memory length: 10000   epsilon: 0.05133744625038449\n",
      "episode: 409   score: -310.0   memory length: 10000   epsilon: 0.05102013717168062\n",
      "episode: 410   score: -243.0   memory length: 10000   epsilon: 0.05072862630436543\n",
      "episode: 411   score: -272.0   memory length: 10000   epsilon: 0.05040903077380856\n",
      "episode: 412   score: -370.0   memory length: 10000   epsilon: 0.05002236985850295\n",
      "episode: 413   score: -64.0   memory length: 10000   epsilon: 0.04967144731679476\n",
      "episode: 414   score: -208.0   memory length: 10000   epsilon: 0.049380234754486566\n",
      "episode: 415   score: -278.0   memory length: 10000   epsilon: 0.04903185566775026\n",
      "episode: 416   score: -86.0   memory length: 10000   epsilon: 0.04872587344494534\n",
      "episode: 417   score: -174.0   memory length: 10000   epsilon: 0.0483598601333311\n",
      "episode: 418   score: 12.0   memory length: 10000   epsilon: 0.04790357259798585\n",
      "episode: 419   score: -320.0   memory length: 10000   epsilon: 0.047602727244305025\n",
      "episode: 420   score: -243.0   memory length: 10000   epsilon: 0.04735914934714458\n",
      "episode: 421   score: 220.0   memory length: 10000   epsilon: 0.04687385125798922\n",
      "episode: 422   score: 974.0   memory length: 10000   epsilon: 0.0464371567452902\n",
      "episode: 423   score: -372.0   memory length: 10000   epsilon: 0.04611691931884342\n",
      "episode: 424   score: -516.0   memory length: 10000   epsilon: 0.04578332124288157\n",
      "episode: 425   score: -187.0   memory length: 10000   epsilon: 0.04557912549344918\n",
      "episode: 426   score: -298.0   memory length: 10000   epsilon: 0.045239463854667906\n",
      "episode: 427   score: 184.0   memory length: 10000   epsilon: 0.04483144339571228\n",
      "episode: 428   score: -237.0   memory length: 10000   epsilon: 0.04456459567740068\n",
      "episode: 429   score: 91.0   memory length: 10000   epsilon: 0.04419226098614904\n",
      "episode: 430   score: 284.0   memory length: 10000   epsilon: 0.0437411642165427\n",
      "episode: 431   score: -415.0   memory length: 10000   epsilon: 0.043329755127261976\n",
      "episode: 432   score: -276.0   memory length: 10000   epsilon: 0.042956137678305986\n",
      "episode: 433   score: 861.0   memory length: 10000   epsilon: 0.04253764685307027\n",
      "episode: 434   score: 212.0   memory length: 10000   epsilon: 0.04218266903197151\n",
      "episode: 435   score: -358.0   memory length: 10000   epsilon: 0.04193955526502328\n",
      "episode: 436   score: 113.0   memory length: 10000   epsilon: 0.041581667227629994\n",
      "episode: 437   score: -341.0   memory length: 10000   epsilon: 0.04127468431490902\n",
      "episode: 438   score: -242.0   memory length: 10000   epsilon: 0.04096136491680274\n",
      "episode: 439   score: -327.0   memory length: 10000   epsilon: 0.040758290974553205\n",
      "episode: 440   score: -293.0   memory length: 10000   epsilon: 0.040432310830037184\n",
      "episode: 441   score: -196.0   memory length: 10000   epsilon: 0.040099713796375794\n",
      "episode: 442   score: -334.0   memory length: 10000   epsilon: 0.0398104388609771\n",
      "episode: 443   score: 336.0   memory length: 10000   epsilon: 0.03940485805688269\n",
      "episode: 444   score: -365.0   memory length: 10000   epsilon: 0.039155428737301726\n",
      "episode: 445   score: -322.0   memory length: 10000   epsilon: 0.03899443943165233\n",
      "episode: 446   score: -481.0   memory length: 10000   epsilon: 0.038633084112856465\n",
      "episode: 447   score: -389.0   memory length: 10000   epsilon: 0.038379327864957256\n",
      "episode: 448   score: -293.0   memory length: 10000   epsilon: 0.037977311861386386\n",
      "episode: 449   score: -364.0   memory length: 10000   epsilon: 0.03770334770066676\n",
      "episode: 450   score: -505.0   memory length: 10000   epsilon: 0.037359933866391096\n",
      "episode: 451   score: -355.0   memory length: 10000   epsilon: 0.037097471287999624\n",
      "episode: 452   score: -270.0   memory length: 10000   epsilon: 0.03680923515306068\n",
      "episode: 453   score: 123.0   memory length: 10000   epsilon: 0.03648053111183445\n",
      "episode: 454   score: -206.0   memory length: 10000   epsilon: 0.036303664979129215\n",
      "episode: 455   score: -382.0   memory length: 10000   epsilon: 0.0360569151236433\n",
      "episode: 456   score: -239.0   memory length: 10000   epsilon: 0.03587385080427358\n",
      "episode: 457   score: -321.0   memory length: 10000   epsilon: 0.035669593803602566\n",
      "episode: 458   score: -555.0   memory length: 10000   epsilon: 0.03531290795104501\n",
      "episode: 459   score: -220.0   memory length: 10000   epsilon: 0.03514381130401605\n",
      "episode: 460   score: -49.0   memory length: 10000   epsilon: 0.03490599194659042\n",
      "episode: 461   score: -267.0   memory length: 10000   epsilon: 0.03449410514704808\n",
      "episode: 462   score: -534.0   memory length: 10000   epsilon: 0.034159761786568384\n",
      "episode: 463   score: -307.0   memory length: 10000   epsilon: 0.0338581030256366\n",
      "episode: 464   score: -366.0   memory length: 10000   epsilon: 0.033515844604544244\n",
      "episode: 465   score: 49.0   memory length: 10000   epsilon: 0.03323183352804937\n",
      "episode: 466   score: -400.0   memory length: 10000   epsilon: 0.032864998045133875\n",
      "episode: 467   score: -241.0   memory length: 10000   epsilon: 0.03258976102929529\n",
      "episode: 468   score: -160.0   memory length: 10000   epsilon: 0.0324694011148399\n",
      "episode: 469   score: 292.0   memory length: 10000   epsilon: 0.03220810410885086\n",
      "episode: 470   score: -331.0   memory length: 10000   epsilon: 0.03190325536784843\n",
      "episode: 471   score: -488.0   memory length: 10000   epsilon: 0.03158012613093619\n",
      "episode: 472   score: -86.0   memory length: 10000   epsilon: 0.03138932831387968\n",
      "episode: 473   score: 514.0   memory length: 10000   epsilon: 0.0310285551611559\n",
      "episode: 474   score: -369.0   memory length: 10000   epsilon: 0.030707835891243575\n",
      "episode: 475   score: -411.0   memory length: 10000   epsilon: 0.030411104284462052\n",
      "episode: 476   score: 303.0   memory length: 10000   epsilon: 0.030166673057282854\n",
      "episode: 477   score: -313.0   memory length: 10000   epsilon: 0.029979317964459462\n",
      "episode: 478   score: -341.0   memory length: 10000   epsilon: 0.029772873930686797\n",
      "episode: 479   score: -299.0   memory length: 10000   epsilon: 0.02960690704520522\n",
      "episode: 480   score: -237.0   memory length: 10000   epsilon: 0.02935132343913753\n",
      "episode: 481   score: -233.0   memory length: 10000   epsilon: 0.02921281880842871\n",
      "episode: 482   score: -247.0   memory length: 10000   epsilon: 0.029033129728794946\n",
      "episode: 483   score: -251.0   memory length: 10000   epsilon: 0.028801502108589748\n",
      "episode: 484   score: -206.0   memory length: 10000   epsilon: 0.028498956983357682\n",
      "episode: 485   score: -241.0   memory length: 10000   epsilon: 0.02825180772977028\n",
      "episode: 486   score: 32.0   memory length: 10000   epsilon: 0.028004841391819735\n",
      "episode: 487   score: -82.0   memory length: 10000   epsilon: 0.02781449723317039\n",
      "episode: 488   score: -145.0   memory length: 10000   epsilon: 0.027613570363602125\n",
      "episode: 489   score: -190.0   memory length: 10000   epsilon: 0.027492336956920296\n",
      "episode: 490   score: 133.0   memory length: 10000   epsilon: 0.027293191442581518\n",
      "episode: 491   score: -183.0   memory length: 10000   epsilon: 0.02718070244914324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 492   score: -308.0   memory length: 10000   epsilon: 0.027042973918716854\n",
      "episode: 493   score: -103.0   memory length: 10000   epsilon: 0.026923437914317276\n",
      "episode: 494   score: -153.0   memory length: 10000   epsilon: 0.026758901237367138\n",
      "episode: 495   score: -229.0   memory length: 10000   epsilon: 0.02650615915891348\n",
      "episode: 496   score: -362.0   memory length: 10000   epsilon: 0.026349705474260497\n",
      "episode: 497   score: -367.0   memory length: 10000   epsilon: 0.026192865581287913\n",
      "episode: 498   score: -521.0   memory length: 10000   epsilon: 0.02592679569149883\n",
      "episode: 499   score: -315.0   memory length: 10000   epsilon: 0.02569578502976694\n",
      "episode: 500   score: 130.0   memory length: 10000   epsilon: 0.025440106424762462\n",
      "episode: 501   score: -446.0   memory length: 10000   epsilon: 0.025208137927745147\n",
      "episode: 502   score: -337.0   memory length: 10000   epsilon: 0.025080655472141885\n",
      "episode: 503   score: -310.0   memory length: 10000   epsilon: 0.02495306911306204\n",
      "episode: 504   score: -218.0   memory length: 10000   epsilon: 0.024821663467304134\n",
      "episode: 505   score: -296.0   memory length: 10000   epsilon: 0.024538829523722997\n",
      "episode: 506   score: -112.0   memory length: 10000   epsilon: 0.02439398808241718\n",
      "episode: 507   score: -266.0   memory length: 10000   epsilon: 0.024278148037972482\n",
      "episode: 508   score: -216.0   memory length: 10000   epsilon: 0.024114580420833\n",
      "episode: 509   score: -178.0   memory length: 10000   epsilon: 0.023939662873186725\n",
      "episode: 510   score: -319.0   memory length: 10000   epsilon: 0.02376577644751956\n",
      "episode: 511   score: -176.0   memory length: 10000   epsilon: 0.02362219062876634\n",
      "episode: 512   score: -171.0   memory length: 10000   epsilon: 0.023480646322058474\n",
      "episode: 513   score: -309.0   memory length: 10000   epsilon: 0.023189195491549554\n",
      "episode: 514   score: -293.0   memory length: 10000   epsilon: 0.023084385485819186\n",
      "episode: 515   score: -50.0   memory length: 10000   epsilon: 0.022941704350976203\n",
      "episode: 516   score: -323.0   memory length: 10000   epsilon: 0.022803781439621047\n",
      "episode: 517   score: -19.0   memory length: 10000   epsilon: 0.022640409460286146\n",
      "episode: 518   score: -268.0   memory length: 10000   epsilon: 0.022424548881440222\n",
      "episode: 519   score: -94.0   memory length: 10000   epsilon: 0.02225610278048278\n",
      "episode: 520   score: -190.0   memory length: 10000   epsilon: 0.022094223997229274\n",
      "episode: 521   score: -62.0   memory length: 10000   epsilon: 0.021878319464331445\n",
      "episode: 522   score: -257.0   memory length: 10000   epsilon: 0.021761147192072266\n",
      "episode: 523   score: -244.0   memory length: 10000   epsilon: 0.021567468601045633\n",
      "episode: 524   score: -174.0   memory length: 10000   epsilon: 0.021469773740260958\n",
      "episode: 525   score: -219.0   memory length: 10000   epsilon: 0.02139069586817187\n",
      "episode: 526   score: 40.0   memory length: 10000   epsilon: 0.02120328263513499\n",
      "episode: 527   score: -346.0   memory length: 10000   epsilon: 0.021033070203603406\n",
      "episode: 528   score: -7.0   memory length: 10000   epsilon: 0.02088489009029661\n",
      "episode: 529   score: -205.0   memory length: 10000   epsilon: 0.02075061537929876\n",
      "episode: 530   score: -83.0   memory length: 10000   epsilon: 0.020617616307667383\n",
      "episode: 531   score: -185.0   memory length: 10000   epsilon: 0.02052401878132791\n",
      "episode: 532   score: -184.0   memory length: 10000   epsilon: 0.020390228999005736\n",
      "episode: 533   score: -196.0   memory length: 10000   epsilon: 0.020230589207132842\n",
      "episode: 534   score: -264.0   memory length: 10000   epsilon: 0.020122845254433416\n",
      "episode: 535   score: -18.0   memory length: 10000   epsilon: 0.019956915164642795\n",
      "episode: 536   score: -204.0   memory length: 10000   epsilon: 0.019866515679901792\n",
      "episode: 537   score: -349.0   memory length: 10000   epsilon: 0.019608156348799203\n",
      "episode: 538   score: 171.0   memory length: 10000   epsilon: 0.019388024147488574\n",
      "episode: 539   score: -305.0   memory length: 10000   epsilon: 0.019296148949751585\n",
      "episode: 540   score: -258.0   memory length: 10000   epsilon: 0.01915043636054162\n",
      "episode: 541   score: -196.0   memory length: 10000   epsilon: 0.019021415347992572\n",
      "episode: 542   score: -148.0   memory length: 10000   epsilon: 0.018845712258326715\n",
      "episode: 543   score: 327.0   memory length: 10000   epsilon: 0.01860546505023185\n",
      "episode: 544   score: -345.0   memory length: 10000   epsilon: 0.018441532337100986\n",
      "episode: 545   score: -305.0   memory length: 10000   epsilon: 0.018310144993446808\n",
      "episode: 546   score: 196.0   memory length: 10000   epsilon: 0.018178057616538925\n",
      "episode: 547   score: -191.0   memory length: 10000   epsilon: 0.018089021670583766\n",
      "episode: 548   score: 135.0   memory length: 10000   epsilon: 0.01788129436611944\n",
      "episode: 549   score: -173.0   memory length: 10000   epsilon: 0.01774005574687871\n",
      "episode: 550   score: -192.0   memory length: 10000   epsilon: 0.017681256129175815\n",
      "episode: 551   score: -266.0   memory length: 10000   epsilon: 0.01757794644472125\n",
      "episode: 552   score: -91.0   memory length: 10000   epsilon: 0.017441196702086558\n",
      "episode: 553   score: -219.0   memory length: 10000   epsilon: 0.017330102224563215\n",
      "episode: 554   score: -167.0   memory length: 10000   epsilon: 0.017175345555146212\n",
      "episode: 555   score: -227.0   memory length: 10000   epsilon: 0.01710900508774789\n",
      "episode: 556   score: -189.0   memory length: 10000   epsilon: 0.016984733340180323\n",
      "episode: 557   score: -191.0   memory length: 10000   epsilon: 0.0168829607170128\n",
      "episode: 558   score: -305.0   memory length: 10000   epsilon: 0.01672751260902014\n",
      "episode: 559   score: -141.0   memory length: 10000   epsilon: 0.0165957192552042\n",
      "episode: 560   score: -175.0   memory length: 10000   epsilon: 0.016487371977047496\n",
      "episode: 561   score: -114.0   memory length: 10000   epsilon: 0.016393005082150792\n",
      "episode: 562   score: -85.0   memory length: 10000   epsilon: 0.01626807642998698\n",
      "episode: 563   score: -401.0   memory length: 10000   epsilon: 0.016147974908816424\n",
      "episode: 564   score: -29.0   memory length: 10000   epsilon: 0.01606277724207126\n",
      "episode: 565   score: -52.0   memory length: 10000   epsilon: 0.01598234375511437\n",
      "episode: 566   score: -270.0   memory length: 10000   epsilon: 0.015826480982014005\n",
      "episode: 567   score: -173.0   memory length: 10000   epsilon: 0.015721897965982952\n",
      "episode: 568   score: -206.0   memory length: 10000   epsilon: 0.015655064802192937\n",
      "episode: 569   score: 165.0   memory length: 10000   epsilon: 0.015534206377004985\n",
      "episode: 570   score: -281.0   memory length: 10000   epsilon: 0.015442669517487347\n",
      "episode: 571   score: -619.0   memory length: 10000   epsilon: 0.015299870731052115\n",
      "episode: 572   score: 140.0   memory length: 10000   epsilon: 0.015164305364447473\n",
      "episode: 573   score: -142.0   memory length: 10000   epsilon: 0.015046182368756362\n",
      "episode: 574   score: -438.0   memory length: 10000   epsilon: 0.014890809996564293\n",
      "episode: 575   score: 26.0   memory length: 10000   epsilon: 0.014762706959703904\n",
      "episode: 576   score: -167.0   memory length: 10000   epsilon: 0.014685111749017904\n",
      "episode: 577   score: -324.0   memory length: 10000   epsilon: 0.014550045607670807\n",
      "episode: 578   score: -203.0   memory length: 10000   epsilon: 0.01447559463147954\n",
      "episode: 579   score: -183.0   memory length: 10000   epsilon: 0.014349772075800472\n",
      "episode: 580   score: -561.0   memory length: 10000   epsilon: 0.014212530581539328\n",
      "episode: 581   score: -274.0   memory length: 10000   epsilon: 0.014081810999881764\n",
      "episode: 582   score: -301.0   memory length: 10000   epsilon: 0.013980645775629942\n",
      "episode: 583   score: -431.0   memory length: 10000   epsilon: 0.013871881665004358\n",
      "episode: 584   score: -246.0   memory length: 10000   epsilon: 0.01374539479081811\n",
      "episode: 585   score: -192.0   memory length: 10000   epsilon: 0.013688879982153988\n",
      "episode: 586   score: -95.0   memory length: 10000   epsilon: 0.013580484288109646\n",
      "episode: 587   score: -311.0   memory length: 10000   epsilon: 0.013465404151908491\n",
      "episode: 588   score: -292.0   memory length: 10000   epsilon: 0.013392618724168862\n",
      "episode: 589   score: -544.0   memory length: 10000   epsilon: 0.013264133530882965\n",
      "episode: 590   score: -232.0   memory length: 10000   epsilon: 0.013205635144855208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 591   score: -236.0   memory length: 10000   epsilon: 0.013114042578158375\n",
      "episode: 592   score: 24.0   memory length: 10000   epsilon: 0.013007466878067607\n",
      "episode: 593   score: -168.0   memory length: 10000   epsilon: 0.012916990436994397\n",
      "episode: 594   score: -229.0   memory length: 10000   epsilon: 0.012859122870499468\n",
      "episode: 595   score: -124.0   memory length: 10000   epsilon: 0.012762529216788922\n",
      "episode: 596   score: -287.0   memory length: 10000   epsilon: 0.012692908420340975\n",
      "episode: 597   score: -270.0   memory length: 10000   epsilon: 0.012594288359803455\n",
      "episode: 598   score: 21.0   memory length: 10000   epsilon: 0.012494060438429998\n",
      "episode: 599   score: -164.0   memory length: 10000   epsilon: 0.01240020901795737\n",
      "episode: 600   score: -445.0   memory length: 10000   epsilon: 0.012266393343829429\n",
      "episode: 601   score: -165.0   memory length: 10000   epsilon: 0.012159530076361795\n",
      "episode: 602   score: -400.0   memory length: 10000   epsilon: 0.012086790967751936\n",
      "episode: 603   score: -144.0   memory length: 10000   epsilon: 0.011976820479163671\n",
      "episode: 604   score: -194.0   memory length: 10000   epsilon: 0.011916608863618087\n",
      "episode: 605   score: -321.0   memory length: 10000   epsilon: 0.011802638188799718\n",
      "episode: 606   score: -312.0   memory length: 10000   epsilon: 0.01171538628206922\n",
      "episode: 607   score: -118.0   memory length: 10000   epsilon: 0.011621106891184124\n",
      "episode: 608   score: 464.0   memory length: 10000   epsilon: 0.011492135691287646\n",
      "episode: 609   score: -90.0   memory length: 10000   epsilon: 0.011435961684242697\n",
      "episode: 610   score: -290.0   memory length: 10000   epsilon: 0.01135618908343202\n",
      "episode: 611   score: -289.0   memory length: 10000   epsilon: 0.011299662568221888\n",
      "episode: 612   score: -108.0   memory length: 10000   epsilon: 0.011231168709666084\n",
      "episode: 613   score: -28.0   memory length: 10000   epsilon: 0.011168673000972958\n",
      "episode: 614   score: -182.0   memory length: 10000   epsilon: 0.011120528160614769\n",
      "episode: 615   score: -296.0   memory length: 10000   epsilon: 0.01106440013115341\n",
      "episode: 616   score: -42.0   memory length: 10000   epsilon: 0.010980409393318765\n",
      "episode: 617   score: -103.0   memory length: 10000   epsilon: 0.010927501626523955\n",
      "episode: 618   score: 258.0   memory length: 10000   epsilon: 0.010807741052543571\n",
      "episode: 619   score: -432.0   memory length: 10000   epsilon: 0.010686086678408079\n",
      "episode: 620   score: -398.0   memory length: 10000   epsilon: 0.010602210800979823\n",
      "episode: 621   score: -120.0   memory length: 10000   epsilon: 0.01052825410601269\n",
      "episode: 622   score: -546.0   memory length: 10000   epsilon: 0.010429125942008451\n",
      "episode: 623   score: 982.0   memory length: 10000   epsilon: 0.010362799592902245\n",
      "episode: 624   score: -197.0   memory length: 10000   epsilon: 0.010288763685545885\n",
      "episode: 625   score: 220.0   memory length: 10000   epsilon: 0.010211886224360724\n",
      "episode: 626   score: -225.0   memory length: 10000   epsilon: 0.01012899716889757\n",
      "episode: 627   score: -40.0   memory length: 10000   epsilon: 0.01002119413494042\n",
      "episode: 628   score: -273.0   memory length: 10000   epsilon: 0.009966926250096281\n",
      "episode: 629   score: -353.0   memory length: 10000   epsilon: 0.009883257801469076\n",
      "episode: 630   score: -515.0   memory length: 10000   epsilon: 0.009814806959699617\n",
      "episode: 631   score: -414.0   memory length: 10000   epsilon: 0.009748877263804401\n",
      "episode: 632   score: -281.0   memory length: 10000   epsilon: 0.009661433966053882\n",
      "episode: 633   score: -329.0   memory length: 10000   epsilon: 0.009603734670014288\n",
      "episode: 634   score: -182.0   memory length: 10000   epsilon: 0.0095308319593982\n",
      "episode: 635   score: -351.0   memory length: 10000   epsilon: 0.00947656571514842\n",
      "episode: 636   score: -457.0   memory length: 10000   epsilon: 0.009419216904239334\n",
      "episode: 637   score: -327.0   memory length: 10000   epsilon: 0.009346312823513721\n",
      "episode: 638   score: -315.0   memory length: 10000   epsilon: 0.009276013523992781\n",
      "episode: 639   score: -373.0   memory length: 10000   epsilon: 0.00921195265724767\n",
      "episode: 640   score: -321.0   memory length: 10000   epsilon: 0.00914668763960846\n",
      "episode: 641   score: -392.0   memory length: 10000   epsilon: 0.00906546158650346\n",
      "episode: 642   score: -370.0   memory length: 10000   epsilon: 0.009004025260776477\n",
      "episode: 643   score: -228.0   memory length: 10000   epsilon: 0.008921746136490792\n",
      "episode: 644   score: -63.0   memory length: 10000   epsilon: 0.008848621122344379\n",
      "episode: 645   score: -372.0   memory length: 10000   epsilon: 0.008791994657055706\n",
      "episode: 646   score: -171.0   memory length: 10000   epsilon: 0.00874193517310564\n",
      "episode: 647   score: -396.0   memory length: 10000   epsilon: 0.008678698235404557\n",
      "episode: 648   score: -456.0   memory length: 10000   epsilon: 0.008600423955927623\n",
      "episode: 649   score: -317.0   memory length: 10000   epsilon: 0.008544958559649325\n",
      "episode: 650   score: -447.0   memory length: 10000   epsilon: 0.008461034246833624\n",
      "episode: 651   score: -301.0   memory length: 10000   epsilon: 0.008410335679316282\n",
      "episode: 652   score: -245.0   memory length: 10000   epsilon: 0.008361278602162656\n",
      "episode: 653   score: 790.0   memory length: 10000   epsilon: 0.008235971121163092\n",
      "episode: 654   score: -355.0   memory length: 10000   epsilon: 0.008197762568139918\n",
      "episode: 655   score: -457.0   memory length: 10000   epsilon: 0.008155489294867799\n",
      "episode: 656   score: -474.0   memory length: 10000   epsilon: 0.008081287371063055\n",
      "episode: 657   score: -294.0   memory length: 10000   epsilon: 0.008038248098437627\n",
      "episode: 658   score: -25.0   memory length: 10000   epsilon: 0.007996157669521334\n",
      "episode: 659   score: -291.0   memory length: 10000   epsilon: 0.007935537558631755\n",
      "episode: 660   score: -339.0   memory length: 10000   epsilon: 0.007881049359813168\n",
      "episode: 661   score: -185.0   memory length: 10000   epsilon: 0.007846056432207054\n",
      "episode: 662   score: -230.0   memory length: 10000   epsilon: 0.007796001766551521\n",
      "episode: 663   score: -332.0   memory length: 10000   epsilon: 0.007745336928563727\n",
      "episode: 664   score: -218.0   memory length: 10000   epsilon: 0.007707631571723974\n",
      "episode: 665   score: -291.0   memory length: 10000   epsilon: 0.007660681287251571\n",
      "episode: 666   score: -188.0   memory length: 10000   epsilon: 0.007619577285315872\n",
      "episode: 667   score: -373.0   memory length: 10000   epsilon: 0.0075722546453000405\n",
      "episode: 668   score: 69.0   memory length: 10000   epsilon: 0.007511091705269441\n",
      "episode: 669   score: -389.0   memory length: 10000   epsilon: 0.007472209930393214\n",
      "episode: 670   score: 229.0   memory length: 10000   epsilon: 0.007398525670105781\n",
      "episode: 671   score: -50.0   memory length: 10000   epsilon: 0.007327839307653588\n",
      "episode: 672   score: -669.0   memory length: 10000   epsilon: 0.007239778689199705\n",
      "episode: 673   score: -358.0   memory length: 10000   epsilon: 0.007193735704249518\n",
      "episode: 674   score: -244.0   memory length: 10000   epsilon: 0.0071461272959669605\n",
      "episode: 675   score: -167.0   memory length: 10000   epsilon: 0.007119948944556688\n",
      "episode: 676   score: -79.0   memory length: 10000   epsilon: 0.007078135497690446\n",
      "episode: 677   score: -224.0   memory length: 10000   epsilon: 0.007036215788030214\n",
      "episode: 678   score: 3.0   memory length: 10000   epsilon: 0.006992236514246057\n",
      "episode: 679   score: -210.0   memory length: 10000   epsilon: 0.006947628874225983\n",
      "episode: 680   score: -125.0   memory length: 10000   epsilon: 0.0069036509885475805\n",
      "episode: 681   score: -207.0   memory length: 10000   epsilon: 0.006835846778487486\n",
      "episode: 682   score: -187.0   memory length: 10000   epsilon: 0.00678633013591266\n",
      "episode: 683   score: -173.0   memory length: 10000   epsilon: 0.006749580046315459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae4ddc240609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;31m# every time step do the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ae4ddc240609>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;31m#         print(update_input, update_target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "# DQN Agent for the MsPacman\n",
    "# it uses Neural Network to approximate q function and replay memory & target q network\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see MsPacman learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.05\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99999\n",
    "        self.epsilon_min = 0.0\n",
    "        self.batch_size = 256\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./Saved Weights/pacman.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "#         total_training_steps += batch_size\n",
    "\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] #STATE\n",
    "            action.append(mini_batch[i][1])    #ACTION\n",
    "            reward.append(mini_batch[i][2])    #REWARD\n",
    "            update_target[i] = mini_batch[i][3]#NEXT STATE\n",
    "            done.append(mini_batch[i][4])      #DONE\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "#         print(update_input, update_target)\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EPISODES = 1000\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "    \n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done: \n",
    "            dead = False         \n",
    "            while not dead:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                reward = reward-1 if not dead else -1000  # if action make Pacman dead, then gives penalty of -100\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                \n",
    "               \n",
    "                \n",
    "                \n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                \n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "\n",
    "                state = next_state            \n",
    "                score += reward\n",
    "                dead = info['ale.lives'] < lives\n",
    "                lives = info['ale.lives']\n",
    "                \n",
    "\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./pacman.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "    #         # save the model\n",
    "        if e % 2 == 0:\n",
    "            agent.model.save_weights(\"./Saved Weights/pacman_correctedv12.h5\")\n",
    "\n",
    "#     print(\"Total Training Steps: {}\".format(total_training_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINDING\n",
    "\n",
    "The Pacman learns to just stay in the corner after consuming the \"Power Pellets\" since the reward from consuming ghosts outweight the negative reward of dying. Must increase the negative reward from dying.\n",
    "\n",
    "The learned weights will be in 'correctedv1.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Test Trained Weights- No Training Involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 25,353\n",
      "Trainable params: 25,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 470.0\n",
      "episode: 1   score: 420.0\n",
      "episode: 2   score: 550.0\n",
      "episode: 3   score: 300.0\n",
      "episode: 4   score: 180.0\n",
      "episode: 5   score: 390.0\n",
      "episode: 6   score: 260.0\n",
      "episode: 7   score: 180.0\n",
      "episode: 8   score: 180.0\n",
      "episode: 9   score: 320.0\n",
      "episode: 10   score: 320.0\n",
      "episode: 11   score: 160.0\n",
      "episode: 12   score: 240.0\n",
      "episode: 13   score: 300.0\n",
      "episode: 14   score: 160.0\n",
      "episode: 15   score: 420.0\n",
      "episode: 16   score: 280.0\n",
      "episode: 17   score: 190.0\n",
      "episode: 18   score: 300.0\n",
      "episode: 19   score: 430.0\n",
      "episode: 20   score: 360.0\n",
      "episode: 21   score: 350.0\n",
      "episode: 22   score: 1870.0\n",
      "episode: 23   score: 190.0\n",
      "episode: 24   score: 240.0\n",
      "episode: 25   score: 410.0\n",
      "episode: 26   score: 470.0\n",
      "episode: 27   score: 250.0\n",
      "episode: 28   score: 330.0\n",
      "episode: 29   score: 900.0\n",
      "episode: 30   score: 190.0\n",
      "episode: 31   score: 1100.0\n",
      "episode: 32   score: 160.0\n",
      "episode: 33   score: 160.0\n",
      "episode: 34   score: 360.0\n",
      "episode: 35   score: 920.0\n",
      "episode: 36   score: 600.0\n",
      "episode: 37   score: 160.0\n",
      "episode: 38   score: 360.0\n",
      "episode: 39   score: 930.0\n",
      "episode: 40   score: 310.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4210e8a3420f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m# get action for the current state and go one step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-4210e8a3420f>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#         q_value = self.model.predict(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1790\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DQN Agent for the MsPacman\n",
    "# it uses Neural Network to approximate q function and replay memory & target q network\n",
    "class TEST_DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see MsPacman learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = True\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./Saved Weights/pacman_correctedv12.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "#         q_value = self.model.predict(state)\n",
    "#         return np.argmax(q_value[0])\n",
    "        \n",
    "        \n",
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EPISODES = 100\n",
    "    ALL_SCORES = np.zeros(EPISODES)\n",
    "\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = TEST_DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done: \n",
    "            dead = False         \n",
    "            while not dead:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "                state = next_state            \n",
    "                score += reward\n",
    "                dead = info['ale.lives']<lives\n",
    "                lives = info['ale.lives']\n",
    "                # if an action make the Pacman dead, then gives penalty of -100\n",
    "                reward = reward if not dead else -500\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./pacman.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "        \n",
    "        ALL_SCORES[e] = score\n",
    "                \n",
    "    env.close()\n",
    "    plt.plot(ALL_SCORES)\n",
    "    plt.title(\"Random Agent: {} Episodes\".format(EPISODES))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "    print('Average Score for {} Episodes: {}'.format(EPISODES, np.mean(ALL_SCORES)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
