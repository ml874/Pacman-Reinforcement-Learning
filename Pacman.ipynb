{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score: 240.0\n",
      "Total Score: 230.0\n",
      "Total Score: 150.0\n",
      "Total Score: 210.0\n",
      "Total Score: 340.0\n",
      "Total Score: 220.0\n",
      "Total Score: 140.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 130.0\n",
      "Total Score: 230.0\n",
      "Total Score: 190.0\n",
      "Total Score: 150.0\n",
      "Total Score: 250.0\n",
      "Total Score: 210.0\n",
      "Total Score: 280.0\n",
      "Total Score: 250.0\n",
      "Total Score: 1010.0\n",
      "Total Score: 110.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 220.0\n",
      "Total Score: 160.0\n",
      "Total Score: 270.0\n",
      "Total Score: 280.0\n",
      "Total Score: 140.0\n",
      "Total Score: 260.0\n",
      "Total Score: 210.0\n",
      "Total Score: 130.0\n",
      "Total Score: 180.0\n",
      "Total Score: 210.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 190.0\n",
      "Total Score: 390.0\n",
      "Total Score: 110.0\n",
      "Total Score: 270.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 330.0\n",
      "Total Score: 190.0\n",
      "Total Score: 320.0\n",
      "Total Score: 280.0\n",
      "Total Score: 190.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 130.0\n",
      "Total Score: 230.0\n",
      "Total Score: 170.0\n",
      "Total Score: 350.0\n",
      "Total Score: 190.0\n",
      "Total Score: 200.0\n",
      "Total Score: 170.0\n",
      "Total Score: 350.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 230.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 190.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 180.0\n",
      "Total Score: 160.0\n",
      "Total Score: 190.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 310.0\n",
      "Total Score: 220.0\n",
      "Total Score: 150.0\n",
      "Total Score: 130.0\n",
      "Total Score: 100.0\n",
      "Total Score: 150.0\n",
      "Total Score: 340.0\n",
      "Total Score: 200.0\n",
      "Total Score: 310.0\n",
      "Total Score: 300.0\n",
      "Total Score: 270.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 130.0\n",
      "Total Score: 200.0\n",
      "Total Score: 490.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 160.0\n",
      "Total Score: 60.0\n",
      "Total Score: 250.0\n",
      "Total Score: 210.0\n",
      "Total Score: 240.0\n",
      "Total Score: 260.0\n",
      "Total Score: 220.0\n",
      "Total Score: 290.0\n",
      "Total Score: 160.0\n",
      "Total Score: 170.0\n",
      "Total Score: 190.0\n",
      "Total Score: 180.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 260.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 250.0\n",
      "Total Score: 200.0\n",
      "Total Score: 120.0\n",
      "Total Score: 290.0\n",
      "Total Score: 160.0\n",
      "Total Score: 160.0\n",
      "Total Score: 140.0\n",
      "Total Score: 170.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 290.0\n",
      "Total Score: 310.0\n",
      "Total Score: 270.0\n",
      "Total Score: 230.0\n",
      "Total Score: 190.0\n",
      "Total Score: 110.0\n",
      "Total Score: 250.0\n",
      "Total Score: 280.0\n",
      "Total Score: 200.0\n",
      "Total Score: 260.0\n",
      "Total Score: 130.0\n",
      "Total Score: 220.0\n",
      "Total Score: 370.0\n",
      "Total Score: 150.0\n",
      "Total Score: 200.0\n",
      "Total Score: 170.0\n",
      "Total Score: 110.0\n",
      "Total Score: 200.0\n",
      "Total Score: 200.0\n",
      "Total Score: 200.0\n",
      "Total Score: 160.0\n",
      "Total Score: 180.0\n",
      "Total Score: 300.0\n",
      "Total Score: 260.0\n",
      "Total Score: 170.0\n",
      "Total Score: 170.0\n",
      "Total Score: 220.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 790.0\n",
      "Total Score: 270.0\n",
      "Total Score: 190.0\n",
      "Total Score: 170.0\n",
      "Total Score: 270.0\n",
      "Total Score: 280.0\n",
      "Total Score: 630.0\n",
      "Total Score: 260.0\n",
      "Total Score: 150.0\n",
      "Total Score: 240.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 230.0\n",
      "Total Score: 100.0\n",
      "Total Score: 230.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 290.0\n",
      "Total Score: 120.0\n",
      "Total Score: 240.0\n",
      "Total Score: 250.0\n",
      "Total Score: 220.0\n",
      "Total Score: 340.0\n",
      "Total Score: 180.0\n",
      "Total Score: 140.0\n",
      "Total Score: 140.0\n",
      "Total Score: 220.0\n",
      "Total Score: 300.0\n",
      "Total Score: 240.0\n",
      "Total Score: 240.0\n",
      "Total Score: 150.0\n",
      "Total Score: 120.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 190.0\n",
      "Total Score: 330.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 280.0\n",
      "Total Score: 250.0\n",
      "Total Score: 240.0\n",
      "Total Score: 210.0\n",
      "Total Score: 280.0\n",
      "Total Score: 280.0\n",
      "Total Score: 390.0\n",
      "Total Score: 110.0\n",
      "Total Score: 190.0\n",
      "Total Score: 110.0\n",
      "Total Score: 170.0\n",
      "Total Score: 150.0\n",
      "Total Score: 130.0\n",
      "Total Score: 140.0\n",
      "Total Score: 160.0\n",
      "Total Score: 180.0\n",
      "Total Score: 190.0\n",
      "Total Score: 650.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 170.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 330.0\n",
      "Total Score: 140.0\n",
      "Total Score: 210.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 230.0\n",
      "Total Score: 180.0\n",
      "Total Score: 330.0\n",
      "Total Score: 290.0\n",
      "Total Score: 190.0\n",
      "Total Score: 280.0\n",
      "Total Score: 220.0\n",
      "Total Score: 300.0\n",
      "Total Score: 570.0\n",
      "Total Score: 610.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 220.0\n",
      "Total Score: 130.0\n",
      "Total Score: 200.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 230.0\n",
      "Total Score: 960.0\n",
      "Total Score: 250.0\n",
      "Total Score: 240.0\n",
      "Total Score: 300.0\n",
      "Total Score: 260.0\n",
      "Total Score: 220.0\n",
      "Total Score: 100.0\n",
      "Total Score: 160.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 180.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 100.0\n",
      "Total Score: 210.0\n",
      "Total Score: 250.0\n",
      "Total Score: 230.0\n",
      "Total Score: 270.0\n",
      "Total Score: 150.0\n",
      "Total Score: 160.0\n",
      "Total Score: 190.0\n",
      "Total Score: 170.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 190.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 190.0\n",
      "Total Score: 290.0\n",
      "Total Score: 300.0\n",
      "Total Score: 280.0\n",
      "Total Score: 190.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 250.0\n",
      "Total Score: 200.0\n",
      "Total Score: 190.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 210.0\n",
      "Total Score: 690.0\n",
      "Total Score: 150.0\n",
      "Total Score: 250.0\n",
      "Total Score: 340.0\n",
      "Total Score: 210.0\n",
      "Total Score: 260.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 90.0\n",
      "Total Score: 150.0\n",
      "Total Score: 170.0\n",
      "Total Score: 110.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 210.0\n",
      "Total Score: 290.0\n",
      "Total Score: 220.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-034a44e5b588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mrandom_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mALL_SCORES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "\n",
    "EPISODES = 500\n",
    "ALL_SCORES = np.zeros(EPISODES)\n",
    "\n",
    "env = gym.make(\"MsPacman-ram-v0\")\n",
    "env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    env.reset()\n",
    "    \n",
    "    reward, info, done = None, None, None\n",
    "\n",
    "    \n",
    "    total_score = 0\n",
    "    while done != True:\n",
    "        env.render()\n",
    "        random_action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(random_action)\n",
    "        total_score += reward\n",
    "    ALL_SCORES[episode] = total_score\n",
    "    print(\"Total Score: {}\".format(total_score))\n",
    "    # print(state, reward, done, info)\n",
    "    \n",
    "env.close()\n",
    "plt.plot(ALL_SCORES)\n",
    "plt.title(\"Random Agent: {} Episodes\".format(EPISODES))\n",
    "plt.show()\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print('Average Score for {} Episodes: {}'.format(EPISODES, np.mean(ALL_SCORES)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2- corrected reward (0.01 LR, batch size 256, min epsilon 0.05, episilon decay 0.9999)  \n",
    "v2- corrected reward (0.05 LR, batch size 256, min epsilon 0.0, episilon decay 0.99999), increased memeory from 2000 to 10000\n",
    "\n",
    "BOTH STILL GET STUCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 50,697\n",
      "Trainable params: 50,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 240.0   memory length: 642   epsilon: 0.9936005322743837\n",
      "episode: 1   score: 210.0   memory length: 1377   epsilon: 0.9863243048735006\n",
      "episode: 2   score: 220.0   memory length: 2015   epsilon: 0.9800515557916739\n",
      "episode: 3   score: 290.0   memory length: 2737   epsilon: 0.9730010313273646\n",
      "episode: 4   score: 250.0   memory length: 3557   epsilon: 0.9650550063670614\n",
      "episode: 5   score: 190.0   memory length: 4271   epsilon: 0.9581890200317489\n",
      "episode: 6   score: 120.0   memory length: 4765   epsilon: 0.9534672151243346\n",
      "episode: 7   score: 270.0   memory length: 5448   epsilon: 0.946977190259284\n",
      "episode: 8   score: 130.0   memory length: 6061   epsilon: 0.9411899471691294\n",
      "episode: 9   score: 260.0   memory length: 6685   epsilon: 0.9353351785008084\n",
      "episode: 10   score: 170.0   memory length: 7214   epsilon: 0.9304002950071845\n",
      "episode: 11   score: 170.0   memory length: 7808   epsilon: 0.9248900712699837\n",
      "episode: 12   score: 180.0   memory length: 8451   epsilon: 0.918962077396477\n",
      "episode: 13   score: 290.0   memory length: 9149   epsilon: 0.9125700243531378\n",
      "episode: 14   score: 120.0   memory length: 9695   epsilon: 0.9076009451058392\n",
      "episode: 15   score: 260.0   memory length: 10273   epsilon: 0.9023701171446411\n",
      "episode: 16   score: 290.0   memory length: 10973   epsilon: 0.8960755515343307\n",
      "episode: 17   score: 220.0   memory length: 11597   epsilon: 0.8905014215945137\n",
      "episode: 18   score: 430.0   memory length: 12680   epsilon: 0.8809092784476475\n",
      "episode: 19   score: 160.0   memory length: 13262   epsilon: 0.8757972513156635\n",
      "episode: 20   score: 150.0   memory length: 13939   epsilon: 0.8698880994271763\n",
      "episode: 21   score: 180.0   memory length: 14470   epsilon: 0.8652812126937789\n",
      "episode: 22   score: 150.0   memory length: 14973   epsilon: 0.8609397544078187\n",
      "episode: 23   score: 270.0   memory length: 15840   epsilon: 0.8535076343723609\n",
      "episode: 24   score: 290.0   memory length: 16488   epsilon: 0.8479947583462912\n",
      "episode: 25   score: 200.0   memory length: 17099   epsilon: 0.8428292811481425\n",
      "episode: 26   score: 200.0   memory length: 17703   epsilon: 0.8377539099639814\n",
      "episode: 27   score: 290.0   memory length: 18380   epsilon: 0.8321014429212793\n",
      "episode: 28   score: 240.0   memory length: 19027   epsilon: 0.8267350985978036\n",
      "episode: 29   score: 190.0   memory length: 19688   epsilon: 0.8212883736201354\n",
      "episode: 30   score: 120.0   memory length: 20100   epsilon: 0.8179116095475453\n",
      "episode: 31   score: 340.0   memory length: 20833   epsilon: 0.8119382068488885\n",
      "episode: 32   score: 190.0   memory length: 21509   epsilon: 0.8064679873926474\n",
      "episode: 33   score: 240.0   memory length: 22250   epsilon: 0.8005141161726513\n",
      "episode: 34   score: 170.0   memory length: 22979   epsilon: 0.7946995590047123\n",
      "episode: 35   score: 320.0   memory length: 23652   epsilon: 0.7893691612283043\n",
      "episode: 36   score: 250.0   memory length: 24282   epsilon: 0.7844117429896397\n",
      "episode: 37   score: 350.0   memory length: 24996   epsilon: 0.7788309623366296\n",
      "episode: 38   score: 350.0   memory length: 25743   epsilon: 0.7730347419031394\n",
      "episode: 39   score: 310.0   memory length: 26408   epsilon: 0.7679110902746742\n",
      "episode: 40   score: 260.0   memory length: 27034   epsilon: 0.7631189579125831\n",
      "episode: 41   score: 250.0   memory length: 27622   epsilon: 0.7586449625070874\n",
      "episode: 42   score: 160.0   memory length: 28178   epsilon: 0.7544385800630652\n",
      "episode: 43   score: 330.0   memory length: 28776   epsilon: 0.7499404775946935\n",
      "episode: 44   score: 360.0   memory length: 29439   epsilon: 0.7449847936950498\n",
      "episode: 45   score: 280.0   memory length: 29983   epsilon: 0.7409430596926501\n",
      "episode: 46   score: 170.0   memory length: 30577   epsilon: 0.7365548817682209\n",
      "episode: 47   score: 240.0   memory length: 31217   epsilon: 0.7318559596202067\n",
      "episode: 48   score: 210.0   memory length: 31816   epsilon: 0.7274852239901934\n",
      "episode: 49   score: 210.0   memory length: 32335   epsilon: 0.7237193377761919\n",
      "episode: 50   score: 1070.0   memory length: 33311   epsilon: 0.7166901600794809\n",
      "episode: 51   score: 220.0   memory length: 33854   epsilon: 0.7128090598281132\n",
      "episode: 52   score: 240.0   memory length: 34383   epsilon: 0.7090482372435403\n",
      "episode: 53   score: 250.0   memory length: 34912   epsilon: 0.7053072569804403\n",
      "episode: 54   score: 490.0   memory length: 35623   epsilon: 0.7003102826927285\n",
      "episode: 55   score: 280.0   memory length: 36339   epsilon: 0.6953139443234122\n",
      "episode: 56   score: 270.0   memory length: 36971   epsilon: 0.6909333954079657\n",
      "episode: 57   score: 220.0   memory length: 37430   epsilon: 0.6877692625425071\n",
      "episode: 58   score: 300.0   memory length: 38137   epsilon: 0.682923858316402\n",
      "episode: 59   score: 120.0   memory length: 38535   epsilon: 0.6802112095489008\n",
      "episode: 60   score: 390.0   memory length: 39610   epsilon: 0.6729380658698829\n",
      "episode: 61   score: 140.0   memory length: 40202   epsilon: 0.6689660215115366\n",
      "episode: 62   score: 190.0   memory length: 40741   epsilon: 0.6653699767221523\n",
      "episode: 63   score: 480.0   memory length: 41588   epsilon: 0.6597580649564907\n",
      "episode: 64   score: 130.0   memory length: 42159   epsilon: 0.65600156264366\n",
      "episode: 65   score: 300.0   memory length: 42761   epsilon: 0.6520642766716623\n",
      "episode: 66   score: 260.0   memory length: 43432   epsilon: 0.6477035501705097\n",
      "episode: 67   score: 240.0   memory length: 44013   epsilon: 0.6439512846691612\n",
      "episode: 68   score: 220.0   memory length: 44583   epsilon: 0.6402911852391735\n",
      "episode: 69   score: 200.0   memory length: 45199   epsilon: 0.636359095148959\n",
      "episode: 70   score: 270.0   memory length: 45898   epsilon: 0.6319264331127922\n",
      "episode: 71   score: 260.0   memory length: 46434   epsilon: 0.6285483518861891\n",
      "episode: 72   score: 290.0   memory length: 47021   epsilon: 0.6248695624769259\n",
      "episode: 73   score: 110.0   memory length: 47485   epsilon: 0.6219768694812081\n",
      "episode: 74   score: 290.0   memory length: 48115   epsilon: 0.6180707129853102\n",
      "episode: 75   score: 250.0   memory length: 48690   epsilon: 0.6145269866439788\n",
      "episode: 76   score: 240.0   memory length: 49208   epsilon: 0.6113519514187258\n",
      "episode: 77   score: 290.0   memory length: 49942   epsilon: 0.606881034080202\n",
      "episode: 78   score: 390.0   memory length: 50755   epsilon: 0.6019670690377783\n",
      "episode: 79   score: 210.0   memory length: 51284   epsilon: 0.5987910553252818\n",
      "episode: 80   score: 200.0   memory length: 51920   epsilon: 0.594994810088465\n",
      "episode: 81   score: 670.0   memory length: 52826   epsilon: 0.5896284764761048\n",
      "episode: 82   score: 910.0   memory length: 53778   epsilon: 0.5840418201245374\n",
      "episode: 83   score: 310.0   memory length: 54339   epsilon: 0.5807745025719229\n",
      "episode: 84   score: 270.0   memory length: 54813   epsilon: 0.578028131738796\n",
      "episode: 85   score: 150.0   memory length: 55310   epsilon: 0.5751624447266153\n",
      "episode: 86   score: 410.0   memory length: 56099   epsilon: 0.570642246069829\n",
      "episode: 87   score: 500.0   memory length: 57164   epsilon: 0.5645971233400062\n",
      "episode: 88   score: 310.0   memory length: 57923   epsilon: 0.5603280315264185\n",
      "episode: 89   score: 270.0   memory length: 58578   epsilon: 0.556669858225389\n",
      "episode: 90   score: 400.0   memory length: 59353   epsilon: 0.5523723198079622\n",
      "episode: 91   score: 340.0   memory length: 59889   epsilon: 0.5494195100094438\n",
      "episode: 92   score: 630.0   memory length: 60867   epsilon: 0.5440723506955286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 93   score: 270.0   memory length: 61504   epsilon: 0.5406176075866189\n",
      "episode: 94   score: 270.0   memory length: 62024   epsilon: 0.5378136785412454\n",
      "episode: 95   score: 330.0   memory length: 62564   epsilon: 0.534917297462358\n",
      "episode: 96   score: 280.0   memory length: 63192   epsilon: 0.5315685262379667\n",
      "episode: 97   score: 270.0   memory length: 63724   epsilon: 0.5287480766242494\n",
      "episode: 98   score: 1000.0   memory length: 64890   epsilon: 0.5226186474202748\n",
      "episode: 99   score: 190.0   memory length: 65488   epsilon: 0.51950269830248\n",
      "episode: 100   score: 210.0   memory length: 65991   epsilon: 0.5168961476678009\n",
      "episode: 101   score: 380.0   memory length: 66652   epsilon: 0.5134907144605512\n",
      "episode: 102   score: 260.0   memory length: 67265   epsilon: 0.5103526287498422\n",
      "episode: 103   score: 290.0   memory length: 67889   epsilon: 0.5071779278410745\n",
      "episode: 104   score: 230.0   memory length: 68437   epsilon: 0.5044061804619976\n",
      "episode: 105   score: 380.0   memory length: 69162   epsilon: 0.5007624419474573\n",
      "episode: 106   score: 270.0   memory length: 69780   epsilon: 0.4976772576689677\n",
      "episode: 107   score: 200.0   memory length: 70233   epsilon: 0.49542786716091186\n",
      "episode: 108   score: 160.0   memory length: 70746   epsilon: 0.4928928174891062\n",
      "episode: 109   score: 350.0   memory length: 71384   epsilon: 0.489758155843777\n",
      "episode: 110   score: 230.0   memory length: 71947   epsilon: 0.4870085510786178\n",
      "episode: 111   score: 300.0   memory length: 72609   epsilon: 0.483795186376172\n",
      "episode: 112   score: 330.0   memory length: 73156   epsilon: 0.481156038161813\n",
      "episode: 113   score: 570.0   memory length: 73945   epsilon: 0.47737463536448943\n",
      "episode: 114   score: 400.0   memory length: 74639   epsilon: 0.4740731084370252\n",
      "episode: 115   score: 240.0   memory length: 75139   epsilon: 0.471708647151722\n",
      "episode: 116   score: 260.0   memory length: 75747   epsilon: 0.46884934536616146\n",
      "episode: 117   score: 570.0   memory length: 76651   epsilon: 0.4646300262441023\n",
      "episode: 118   score: 630.0   memory length: 77558   epsilon: 0.46043486474728185\n",
      "episode: 119   score: 310.0   memory length: 78213   epsilon: 0.4574288567763918\n",
      "episode: 120   score: 210.0   memory length: 78725   epsilon: 0.45509279477133885\n",
      "episode: 121   score: 220.0   memory length: 79306   epsilon: 0.4524563587146218\n",
      "episode: 122   score: 300.0   memory length: 79920   epsilon: 0.4496867741498518\n",
      "episode: 123   score: 290.0   memory length: 80594   epsilon: 0.44666606142574666\n",
      "episode: 124   score: 460.0   memory length: 81326   epsilon: 0.44340838720161346\n",
      "episode: 125   score: 280.0   memory length: 81915   epsilon: 0.4408043751247456\n",
      "episode: 126   score: 220.0   memory length: 82532   epsilon: 0.43809297185400536\n",
      "episode: 127   score: 300.0   memory length: 83025   epsilon: 0.43593847792158036\n",
      "episode: 128   score: 360.0   memory length: 83732   epsilon: 0.43286724712037483\n",
      "episode: 129   score: 290.0   memory length: 84435   epsilon: 0.42983484658769294\n",
      "episode: 130   score: 250.0   memory length: 85047   epsilon: 0.42721227746084744\n",
      "episode: 131   score: 250.0   memory length: 85584   epsilon: 0.42492428484931616\n",
      "episode: 132   score: 270.0   memory length: 86212   epsilon: 0.42226410873536524\n",
      "episode: 133   score: 230.0   memory length: 86801   epsilon: 0.4197842710270306\n",
      "episode: 134   score: 1560.0   memory length: 88084   epsilon: 0.414432815070686\n",
      "episode: 135   score: 300.0   memory length: 88710   epsilon: 0.4118465561532605\n",
      "episode: 136   score: 320.0   memory length: 89315   epsilon: 0.4093623942347618\n",
      "episode: 137   score: 280.0   memory length: 89860   epsilon: 0.4071374266054088\n",
      "episode: 138   score: 560.0   memory length: 90774   epsilon: 0.4034331264445284\n",
      "episode: 139   score: 170.0   memory length: 91293   epsilon: 0.40134472217314665\n",
      "episode: 140   score: 500.0   memory length: 92194   epsilon: 0.39774483009409767\n",
      "episode: 141   score: 390.0   memory length: 92896   epsilon: 0.39496242514273155\n",
      "episode: 142   score: 560.0   memory length: 93790   epsilon: 0.3914471800439814\n",
      "episode: 143   score: 280.0   memory length: 94408   epsilon: 0.38903548422862855\n",
      "episode: 144   score: 370.0   memory length: 95026   epsilon: 0.3866386468079774\n",
      "episode: 145   score: 350.0   memory length: 95603   epsilon: 0.38441415452533817\n",
      "episode: 146   score: 550.0   memory length: 96402   epsilon: 0.3813549080991898\n",
      "episode: 147   score: 410.0   memory length: 97112   epsilon: 0.37865686415154964\n",
      "episode: 148   score: 250.0   memory length: 97759   epsilon: 0.3762148504634696\n",
      "episode: 149   score: 930.0   memory length: 98894   epsilon: 0.37196893185018187\n",
      "episode: 150   score: 690.0   memory length: 99968   epsilon: 0.3679953420269525\n",
      "episode: 151   score: 330.0   memory length: 100560   epsilon: 0.3658232345235569\n",
      "episode: 152   score: 540.0   memory length: 101464   epsilon: 0.3625310789858048\n",
      "episode: 153   score: 320.0   memory length: 102162   epsilon: 0.36000941030791833\n",
      "episode: 154   score: 810.0   memory length: 103073   epsilon: 0.35674460203715536\n",
      "episode: 155   score: 510.0   memory length: 103933   epsilon: 0.35368973792918507\n",
      "episode: 156   score: 590.0   memory length: 104882   epsilon: 0.3503490820966083\n",
      "episode: 157   score: 270.0   memory length: 105474   epsilon: 0.3482811323615989\n",
      "episode: 158   score: 650.0   memory length: 106267   epsilon: 0.34553017120436064\n",
      "episode: 159   score: 200.0   memory length: 106808   epsilon: 0.3436658910814843\n",
      "episode: 160   score: 400.0   memory length: 107533   epsilon: 0.34118331118462303\n",
      "episode: 161   score: 590.0   memory length: 108423   epsilon: 0.3381602372336613\n",
      "episode: 162   score: 360.0   memory length: 109080   epsilon: 0.3359457958084487\n",
      "episode: 163   score: 1260.0   memory length: 109979   epsilon: 0.33293916313459526\n",
      "episode: 164   score: 330.0   memory length: 110513   epsilon: 0.3311659977029179\n",
      "episode: 165   score: 550.0   memory length: 111445   epsilon: 0.3280938536726727\n",
      "episode: 166   score: 260.0   memory length: 112099   epsilon: 0.32595511048929404\n",
      "episode: 167   score: 730.0   memory length: 113117   epsilon: 0.3226537036295349\n",
      "episode: 168   score: 610.0   memory length: 114091   epsilon: 0.3195262961179363\n",
      "episode: 169   score: 610.0   memory length: 114929   epsilon: 0.3168598404775594\n",
      "episode: 170   score: 630.0   memory length: 115821   epsilon: 0.3140460049948635\n",
      "episode: 171   score: 400.0   memory length: 116467   epsilon: 0.31202379645765105\n",
      "episode: 172   score: 530.0   memory length: 117534   epsilon: 0.3087121848579705\n",
      "episode: 173   score: 490.0   memory length: 118362   epsilon: 0.30616658855162027\n",
      "episode: 174   score: 840.0   memory length: 119344   epsilon: 0.303174731752375\n",
      "episode: 175   score: 670.0   memory length: 120124   epsilon: 0.3008191557560212\n",
      "episode: 176   score: 800.0   memory length: 121487   epsilon: 0.29674678654351866\n",
      "episode: 177   score: 800.0   memory length: 122474   epsilon: 0.29383228789883936\n",
      "episode: 178   score: 320.0   memory length: 122973   epsilon: 0.2923697096369669\n",
      "episode: 179   score: 1020.0   memory length: 123841   epsilon: 0.28984291009796476\n",
      "episode: 180   score: 920.0   memory length: 125169   epsilon: 0.28601922261337637\n",
      "episode: 181   score: 200.0   memory length: 125642   epsilon: 0.28466953945902596\n",
      "episode: 182   score: 500.0   memory length: 126459   epsilon: 0.2823532526559764\n",
      "episode: 183   score: 400.0   memory length: 127278   epsilon: 0.28005021182700535\n",
      "episode: 184   score: 350.0   memory length: 127836   epsilon: 0.27849187565467737\n",
      "episode: 185   score: 360.0   memory length: 128371   epsilon: 0.27700591517873124\n",
      "episode: 186   score: 410.0   memory length: 129142   epsilon: 0.2748784010411804\n",
      "episode: 187   score: 350.0   memory length: 129765   epsilon: 0.27317122344673883\n",
      "episode: 188   score: 290.0   memory length: 130285   epsilon: 0.27175441290073804\n",
      "episode: 189   score: 240.0   memory length: 130805   epsilon: 0.2703449506841032\n",
      "episode: 190   score: 280.0   memory length: 131398   epsilon: 0.26874654110291457\n",
      "episode: 191   score: 380.0   memory length: 132101   epsilon: 0.2668638688521998\n",
      "episode: 192   score: 280.0   memory length: 132672   epsilon: 0.2653444107450526\n",
      "episode: 193   score: 620.0   memory length: 133464   epsilon: 0.263251172710189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 194   score: 920.0   memory length: 134276   epsilon: 0.2611222177976033\n",
      "episode: 195   score: 180.0   memory length: 134673   epsilon: 0.2600876124703567\n",
      "episode: 196   score: 1890.0   memory length: 135607   epsilon: 0.257669691349366\n",
      "episode: 197   score: 550.0   memory length: 136426   epsilon: 0.2555679843069159\n",
      "episode: 198   score: 570.0   memory length: 137188   epsilon: 0.25362794749385814\n",
      "episode: 199   score: 1090.0   memory length: 138184   epsilon: 0.25111433911804726\n",
      "episode: 200   score: 380.0   memory length: 138948   epsilon: 0.24920312614111714\n",
      "episode: 201   score: 400.0   memory length: 139594   epsilon: 0.24759845459246715\n",
      "episode: 202   score: 350.0   memory length: 140116   epsilon: 0.2463093516997044\n",
      "episode: 203   score: 310.0   memory length: 140759   epsilon: 0.2447306556219549\n",
      "episode: 204   score: 410.0   memory length: 141491   epsilon: 0.24294575898993873\n",
      "episode: 205   score: 350.0   memory length: 142154   epsilon: 0.24134034839765459\n",
      "episode: 206   score: 390.0   memory length: 142794   epsilon: 0.23980069461710687\n",
      "episode: 207   score: 410.0   memory length: 143423   epsilon: 0.23829707457236904\n",
      "episode: 208   score: 300.0   memory length: 143997   epsilon: 0.23693316072232254\n",
      "episode: 209   score: 350.0   memory length: 144796   epsilon: 0.23504759819388046\n",
      "episode: 210   score: 440.0   memory length: 145603   epsilon: 0.23315838784707285\n",
      "episode: 211   score: 350.0   memory length: 146308   epsilon: 0.23152039373640904\n",
      "episode: 212   score: 360.0   memory length: 146918   epsilon: 0.23011241100795876\n",
      "episode: 213   score: 290.0   memory length: 147504   epsilon: 0.22876788885429292\n",
      "episode: 214   score: 380.0   memory length: 148537   epsilon: 0.2264168687325454\n",
      "episode: 215   score: 350.0   memory length: 149058   epsilon: 0.225240298590238\n",
      "episode: 216   score: 280.0   memory length: 149605   epsilon: 0.22401159159073541\n",
      "episode: 217   score: 190.0   memory length: 150065   epsilon: 0.22298349955351693\n",
      "episode: 218   score: 300.0   memory length: 150660   epsilon: 0.22166068040664408\n",
      "episode: 219   score: 410.0   memory length: 151411   epsilon: 0.2200022356593224\n",
      "episode: 220   score: 320.0   memory length: 152037   epsilon: 0.21862931651987066\n",
      "episode: 221   score: 430.0   memory length: 152737   epsilon: 0.217104247647326\n",
      "episode: 222   score: 420.0   memory length: 153463   epsilon: 0.21553377068641813\n",
      "episode: 223   score: 290.0   memory length: 154256   epsilon: 0.21383133843804777\n",
      "episode: 224   score: 1380.0   memory length: 155299   epsilon: 0.21161265702162715\n",
      "episode: 225   score: 410.0   memory length: 155913   epsilon: 0.21031732955565421\n",
      "episode: 226   score: 700.0   memory length: 156847   epsilon: 0.20836210105240924\n",
      "episode: 227   score: 1360.0   memory length: 158185   epsilon: 0.20559277043042176\n",
      "episode: 228   score: 420.0   memory length: 158885   epsilon: 0.20415863918217655\n",
      "episode: 229   score: 330.0   memory length: 159556   epsilon: 0.20279331367643766\n",
      "episode: 230   score: 300.0   memory length: 160173   epsilon: 0.2015459248458909\n",
      "episode: 231   score: 260.0   memory length: 160709   epsilon: 0.20046852331725679\n",
      "episode: 232   score: 1810.0   memory length: 161618   epsilon: 0.198654512539959\n",
      "episode: 233   score: 620.0   memory length: 162489   epsilon: 0.19693173670206796\n",
      "episode: 234   score: 890.0   memory length: 163598   epsilon: 0.19475981842933882\n",
      "episode: 235   score: 390.0   memory length: 164276   epsilon: 0.1934438066016872\n",
      "episode: 236   score: 380.0   memory length: 164912   epsilon: 0.19221740196041462\n",
      "episode: 237   score: 1090.0   memory length: 166004   epsilon: 0.190129796563454\n",
      "episode: 238   score: 490.0   memory length: 166680   epsilon: 0.18884884722075843\n",
      "episode: 239   score: 380.0   memory length: 167275   epsilon: 0.18772852723533895\n",
      "episode: 240   score: 640.0   memory length: 168135   epsilon: 0.18612097624054158\n",
      "episode: 241   score: 350.0   memory length: 168737   epsilon: 0.18500388818682842\n",
      "episode: 242   score: 240.0   memory length: 169185   epsilon: 0.18417692042383624\n",
      "episode: 243   score: 410.0   memory length: 169803   epsilon: 0.18304221124488468\n",
      "episode: 244   score: 1420.0   memory length: 170696   epsilon: 0.18141491286334527\n",
      "episode: 245   score: 540.0   memory length: 171563   epsilon: 0.1798488364895696\n",
      "episode: 246   score: 540.0   memory length: 172412   epsilon: 0.178328375754233\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "# DQN Agent for the MsPacman\n",
    "# it uses Neural Network to approximate q function and replay memory & target q network\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see MsPacman learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.005\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99999\n",
    "        self.epsilon_min = 0.0\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "\n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./Saved Weights/pacman.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "#         total_training_steps += batch_size\n",
    "\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] #STATE\n",
    "            action.append(mini_batch[i][1])    #ACTION\n",
    "            reward.append(mini_batch[i][2])    #REWARD\n",
    "            update_target[i] = mini_batch[i][3]#NEXT STATE\n",
    "            done.append(mini_batch[i][4])      #DONE\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "#                 print(np.amax(target_val[i]))\n",
    "#         print(update_input, update_target)\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EPISODES = 1000\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "    \n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])/256.0\n",
    "#         print(state)\n",
    "        lives = 3\n",
    "        while not done: \n",
    "            dead = False         \n",
    "            while not dead:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                \n",
    "                dead = info['ale.lives'] != lives\n",
    "                lives = info['ale.lives']\n",
    "#                 if (dead):\n",
    "#                     print(dead, done)\n",
    "                reward = reward - 2 if not dead else -10000  # if action make Pacman dead, then gives penalty of -100\n",
    "#                 print(reward)\n",
    "                next_state = np.reshape(next_state, [1, state_size])/256.0\n",
    "                \n",
    "               \n",
    "                \n",
    "                \n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                \n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "                \n",
    "                state = next_state            \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./pacman.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "#     #   # save the model\n",
    "        if e % 10 == 0:\n",
    "            agent.model.save_weights(\"./Saved Weights/pacman_correctedv22.h5\")\n",
    "\n",
    "#     print(\"Total Training Steps: {}\".format(total_training_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINDING\n",
    "\n",
    "The Pacman learns to just stay in the corner after consuming the \"Power Pellets\" since the reward from consuming ghosts outweight the negative reward of dying. Must increase the negative reward from dying.\n",
    "\n",
    "The learned weights will be in 'correctedv1.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Test Trained Weights- No Training Involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 50,697\n",
      "Trainable params: 50,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 240.0\n",
      "episode: 1   score: 240.0\n",
      "episode: 2   score: 250.0\n",
      "episode: 3   score: 850.0\n",
      "episode: 4   score: 840.0\n",
      "episode: 5   score: 250.0\n",
      "episode: 6   score: 920.0\n",
      "episode: 7   score: 500.0\n",
      "episode: 8   score: 320.0\n",
      "episode: 9   score: 240.0\n",
      "episode: 10   score: 240.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-db4c73c5f726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# get action for the current state and go one step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mblit\u001b[0;34m(self, x, y, z, width, height)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblit_to_texture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternalformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mget_texture\u001b[0;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[1;32m    816\u001b[0m             self._current_texture = self.create_texture(Texture, \n\u001b[1;32m    817\u001b[0m                                                         \u001b[0mrectangle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m                                                         force_rectangle)\n\u001b[0m\u001b[1;32m    819\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_texture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mcreate_texture\u001b[0;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0minternalformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_internalformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         texture = cls.create(self.width, self.height, internalformat, \n\u001b[0;32m--> 803\u001b[0;31m                              rectangle, force_rectangle)\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtexture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mget_region\u001b[0;34m(self, x, y, width, height)\u001b[0m\n\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflip_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflip_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, z, width, height, owner)\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1755\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1757\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DQN Agent for the MsPacman\n",
    "# it uses Neural Network to approximate q function and replay memory & target q network\n",
    "class TEST_DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see MsPacman learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = True\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./Saved Weights/pacman_correctedv15.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation=None, kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "#         q_value = self.model.predict(state)\n",
    "#         return np.argmax(q_value[0])\n",
    "        \n",
    "        \n",
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EPISODES = 100\n",
    "    ALL_SCORES = np.zeros(EPISODES)\n",
    "\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = TEST_DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done: \n",
    "            dead = False         \n",
    "            while not dead:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "                state = next_state            \n",
    "                score += reward\n",
    "                dead = info['ale.lives']<lives\n",
    "                lives = info['ale.lives']\n",
    "                # if an action make the Pacman dead, then gives penalty of -100\n",
    "                reward = reward if not dead else -500\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./pacman.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "        \n",
    "        ALL_SCORES[e] = score\n",
    "                \n",
    "    env.close()\n",
    "    plt.plot(ALL_SCORES)\n",
    "    plt.title(\"Random Agent: {} Episodes\".format(EPISODES))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "    print('Average Score for {} Episodes: {}'.format(EPISODES, np.mean(ALL_SCORES)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
