{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score: 350.0\n",
      "Total Score: 250.0\n",
      "Total Score: 160.0\n",
      "Total Score: 170.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "\n",
    "EPISODES = 500\n",
    "ALL_SCORES = np.zeros(EPISODES)\n",
    "\n",
    "env = gym.make(\"MsPacman-ram-v0\")\n",
    "env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    env.reset()\n",
    "    \n",
    "    reward, info, done = None, None, None\n",
    "\n",
    "    \n",
    "    total_score = 0\n",
    "    while done != True:\n",
    "        env.render()\n",
    "        random_action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(random_action)\n",
    "        total_score += reward\n",
    "    ALL_SCORES[episode] = total_score\n",
    "    print(\"Total Score: {}\".format(total_score))\n",
    "    # print(state, reward, done, info)\n",
    "    \n",
    "env.close()\n",
    "plt.plot(ALL_SCORES)\n",
    "plt.title(\"Random Agent: {} Episodes\".format(EPISODES))\n",
    "plt.show()\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print('Average Score for {} Episodes: {}'.format(EPISODES, np.mean(ALL_SCORES)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2- corrected reward (0.01 LR, batch size 256, min epsilon 0.05, episilon decay 0.9999)  \n",
    "v2- corrected reward (0.05 LR, batch size 256, min epsilon 0.0, episilon decay 0.99999), increased memeory from 2000 to 10000\n",
    "\n",
    "BOTH STILL GET STUCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 25,353\n",
      "Trainable params: 25,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 440.0   memory length: 1060   epsilon: 0.9894559295807855\n",
      "episode: 1   score: 170.0   memory length: 1688   epsilon: 0.9832615859682241\n",
      "episode: 2   score: 120.0   memory length: 2185   epsilon: 0.9783868752028313\n",
      "episode: 3   score: 270.0   memory length: 2860   epsilon: 0.9718049698071488\n",
      "episode: 4   score: 240.0   memory length: 3567   epsilon: 0.9649585052090569\n",
      "episode: 5   score: 650.0   memory length: 4615   epsilon: 0.9548984962959446\n",
      "episode: 6   score: 290.0   memory length: 5323   epsilon: 0.9481616578074399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6164448a8cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;31m# every time step do the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6164448a8cdf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#DONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mtarget_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1790\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "# DQN Agent for the MsPacman\n",
    "# it uses Neural Network to approximate q function and replay memory & target q network\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see MsPacman learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.05\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99999\n",
    "        self.epsilon_min = 0.0\n",
    "        self.batch_size = 256\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./Saved Weights/pacman.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "#         total_training_steps += batch_size\n",
    "\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] #STATE\n",
    "            action.append(mini_batch[i][1])    #ACTION\n",
    "            reward.append(mini_batch[i][2])    #REWARD\n",
    "            update_target[i] = mini_batch[i][3]#NEXT STATE\n",
    "            done.append(mini_batch[i][4])      #DONE\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "#         print(update_input, update_target)\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EPISODES = 1000\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "    \n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done: \n",
    "            dead = False         \n",
    "            while not dead:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                reward = reward if not dead else -1000  # if action make Pacman dead, then gives penalty of -100\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                \n",
    "               \n",
    "                \n",
    "                \n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                \n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "\n",
    "                state = next_state            \n",
    "                score += reward\n",
    "                dead = info['ale.lives'] < lives\n",
    "                lives = info['ale.lives']\n",
    "                \n",
    "\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./pacman.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "    #         # save the model\n",
    "        if e % 2 == 0:\n",
    "            agent.model.save_weights(\"./Saved Weights/pacman_correctedv11.h5\")\n",
    "\n",
    "#     print(\"Total Training Steps: {}\".format(total_training_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINDING\n",
    "\n",
    "The Pacman learns to just stay in the corner after consuming the \"Power Pellets\" since the reward from consuming ghosts outweight the negative reward of dying. Must increase the negative reward from dying.\n",
    "\n",
    "The learned weights will be in 'correctedv1.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Test Trained Weights- No Training Involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 25,353\n",
      "Trainable params: 25,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 460.0\n",
      "episode: 1   score: 560.0\n",
      "episode: 2   score: 440.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e09061fa5ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m# get action for the current state and go one step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mblit\u001b[0;34m(self, x, y, z, width, height)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblit_to_texture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternalformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mget_texture\u001b[0;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[1;32m    816\u001b[0m             self._current_texture = self.create_texture(Texture, \n\u001b[1;32m    817\u001b[0m                                                         \u001b[0mrectangle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m                                                         force_rectangle)\n\u001b[0m\u001b[1;32m    819\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_texture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mcreate_texture\u001b[0;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0minternalformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_internalformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         texture = cls.create(self.width, self.height, internalformat, \n\u001b[0;32m--> 803\u001b[0;31m                              rectangle, force_rectangle)\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[1;32m   1524\u001b[0m                                   0., height, 0.)\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1526\u001b[0;31m         \u001b[0mglFlush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtexture_width\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtexture_height\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DQN Agent for the MsPacman\n",
    "# it uses Neural Network to approximate q function and replay memory & target q network\n",
    "class TEST_DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see MsPacman learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = True\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./Saved Weights/pacman_correctedv10.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "#         q_value = self.model.predict(state)\n",
    "#         return np.argmax(q_value[0])\n",
    "        \n",
    "        \n",
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EPISODES = 100\n",
    "    ALL_SCORES = np.zeros(EPISODES)\n",
    "\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = TEST_DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done: \n",
    "            dead = False         \n",
    "            while not dead:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "                state = next_state            \n",
    "                score += reward\n",
    "                dead = info['ale.lives']<lives\n",
    "                lives = info['ale.lives']\n",
    "                # if an action make the Pacman dead, then gives penalty of -100\n",
    "                reward = reward if not dead else -500\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./pacman.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "        \n",
    "        ALL_SCORES[e] = score\n",
    "                \n",
    "    env.close()\n",
    "    plt.plot(ALL_SCORES)\n",
    "    plt.title(\"Random Agent: {} Episodes\".format(EPISODES))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "    print('Average Score for {} Episodes: {}'.format(EPISODES, np.mean(ALL_SCORES)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
