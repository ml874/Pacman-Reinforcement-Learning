{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score: 240.0\n",
      "Total Score: 230.0\n",
      "Total Score: 150.0\n",
      "Total Score: 210.0\n",
      "Total Score: 340.0\n",
      "Total Score: 220.0\n",
      "Total Score: 140.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 130.0\n",
      "Total Score: 230.0\n",
      "Total Score: 190.0\n",
      "Total Score: 150.0\n",
      "Total Score: 250.0\n",
      "Total Score: 210.0\n",
      "Total Score: 280.0\n",
      "Total Score: 250.0\n",
      "Total Score: 1010.0\n",
      "Total Score: 110.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 220.0\n",
      "Total Score: 160.0\n",
      "Total Score: 270.0\n",
      "Total Score: 280.0\n",
      "Total Score: 140.0\n",
      "Total Score: 260.0\n",
      "Total Score: 210.0\n",
      "Total Score: 130.0\n",
      "Total Score: 180.0\n",
      "Total Score: 210.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 190.0\n",
      "Total Score: 390.0\n",
      "Total Score: 110.0\n",
      "Total Score: 270.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 330.0\n",
      "Total Score: 190.0\n",
      "Total Score: 320.0\n",
      "Total Score: 280.0\n",
      "Total Score: 190.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 130.0\n",
      "Total Score: 230.0\n",
      "Total Score: 170.0\n",
      "Total Score: 350.0\n",
      "Total Score: 190.0\n",
      "Total Score: 200.0\n",
      "Total Score: 170.0\n",
      "Total Score: 350.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 230.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 190.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 180.0\n",
      "Total Score: 160.0\n",
      "Total Score: 190.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 310.0\n",
      "Total Score: 220.0\n",
      "Total Score: 150.0\n",
      "Total Score: 130.0\n",
      "Total Score: 100.0\n",
      "Total Score: 150.0\n",
      "Total Score: 340.0\n",
      "Total Score: 200.0\n",
      "Total Score: 310.0\n",
      "Total Score: 300.0\n",
      "Total Score: 270.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 130.0\n",
      "Total Score: 200.0\n",
      "Total Score: 490.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 160.0\n",
      "Total Score: 60.0\n",
      "Total Score: 250.0\n",
      "Total Score: 210.0\n",
      "Total Score: 240.0\n",
      "Total Score: 260.0\n",
      "Total Score: 220.0\n",
      "Total Score: 290.0\n",
      "Total Score: 160.0\n",
      "Total Score: 170.0\n",
      "Total Score: 190.0\n",
      "Total Score: 180.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 260.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 250.0\n",
      "Total Score: 200.0\n",
      "Total Score: 120.0\n",
      "Total Score: 290.0\n",
      "Total Score: 160.0\n",
      "Total Score: 160.0\n",
      "Total Score: 140.0\n",
      "Total Score: 170.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 290.0\n",
      "Total Score: 310.0\n",
      "Total Score: 270.0\n",
      "Total Score: 230.0\n",
      "Total Score: 190.0\n",
      "Total Score: 110.0\n",
      "Total Score: 250.0\n",
      "Total Score: 280.0\n",
      "Total Score: 200.0\n",
      "Total Score: 260.0\n",
      "Total Score: 130.0\n",
      "Total Score: 220.0\n",
      "Total Score: 370.0\n",
      "Total Score: 150.0\n",
      "Total Score: 200.0\n",
      "Total Score: 170.0\n",
      "Total Score: 110.0\n",
      "Total Score: 200.0\n",
      "Total Score: 200.0\n",
      "Total Score: 200.0\n",
      "Total Score: 160.0\n",
      "Total Score: 180.0\n",
      "Total Score: 300.0\n",
      "Total Score: 260.0\n",
      "Total Score: 170.0\n",
      "Total Score: 170.0\n",
      "Total Score: 220.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 790.0\n",
      "Total Score: 270.0\n",
      "Total Score: 190.0\n",
      "Total Score: 170.0\n",
      "Total Score: 270.0\n",
      "Total Score: 280.0\n",
      "Total Score: 630.0\n",
      "Total Score: 260.0\n",
      "Total Score: 150.0\n",
      "Total Score: 240.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 230.0\n",
      "Total Score: 100.0\n",
      "Total Score: 230.0\n",
      "Total Score: 180.0\n",
      "Total Score: 200.0\n",
      "Total Score: 290.0\n",
      "Total Score: 120.0\n",
      "Total Score: 240.0\n",
      "Total Score: 250.0\n",
      "Total Score: 220.0\n",
      "Total Score: 340.0\n",
      "Total Score: 180.0\n",
      "Total Score: 140.0\n",
      "Total Score: 140.0\n",
      "Total Score: 220.0\n",
      "Total Score: 300.0\n",
      "Total Score: 240.0\n",
      "Total Score: 240.0\n",
      "Total Score: 150.0\n",
      "Total Score: 120.0\n",
      "Total Score: 150.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 190.0\n",
      "Total Score: 330.0\n",
      "Total Score: 220.0\n",
      "Total Score: 210.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 280.0\n",
      "Total Score: 250.0\n",
      "Total Score: 240.0\n",
      "Total Score: 210.0\n",
      "Total Score: 280.0\n",
      "Total Score: 280.0\n",
      "Total Score: 390.0\n",
      "Total Score: 110.0\n",
      "Total Score: 190.0\n",
      "Total Score: 110.0\n",
      "Total Score: 170.0\n",
      "Total Score: 150.0\n",
      "Total Score: 130.0\n",
      "Total Score: 140.0\n",
      "Total Score: 160.0\n",
      "Total Score: 180.0\n",
      "Total Score: 190.0\n",
      "Total Score: 650.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 170.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 330.0\n",
      "Total Score: 140.0\n",
      "Total Score: 210.0\n",
      "Total Score: 240.0\n",
      "Total Score: 200.0\n",
      "Total Score: 230.0\n",
      "Total Score: 180.0\n",
      "Total Score: 330.0\n",
      "Total Score: 290.0\n",
      "Total Score: 190.0\n",
      "Total Score: 280.0\n",
      "Total Score: 220.0\n",
      "Total Score: 300.0\n",
      "Total Score: 570.0\n",
      "Total Score: 610.0\n",
      "Total Score: 240.0\n",
      "Total Score: 180.0\n",
      "Total Score: 220.0\n",
      "Total Score: 130.0\n",
      "Total Score: 200.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 230.0\n",
      "Total Score: 960.0\n",
      "Total Score: 250.0\n",
      "Total Score: 240.0\n",
      "Total Score: 300.0\n",
      "Total Score: 260.0\n",
      "Total Score: 220.0\n",
      "Total Score: 100.0\n",
      "Total Score: 160.0\n",
      "Total Score: 210.0\n",
      "Total Score: 200.0\n",
      "Total Score: 180.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 100.0\n",
      "Total Score: 210.0\n",
      "Total Score: 250.0\n",
      "Total Score: 230.0\n",
      "Total Score: 270.0\n",
      "Total Score: 150.0\n",
      "Total Score: 160.0\n",
      "Total Score: 190.0\n",
      "Total Score: 170.0\n",
      "Total Score: 180.0\n",
      "Total Score: 170.0\n",
      "Total Score: 210.0\n",
      "Total Score: 210.0\n",
      "Total Score: 190.0\n",
      "Total Score: 270.0\n",
      "Total Score: 220.0\n",
      "Total Score: 200.0\n",
      "Total Score: 240.0\n",
      "Total Score: 190.0\n",
      "Total Score: 290.0\n",
      "Total Score: 300.0\n",
      "Total Score: 280.0\n",
      "Total Score: 190.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 250.0\n",
      "Total Score: 200.0\n",
      "Total Score: 190.0\n",
      "Total Score: 220.0\n",
      "Total Score: 240.0\n",
      "Total Score: 210.0\n",
      "Total Score: 690.0\n",
      "Total Score: 150.0\n",
      "Total Score: 250.0\n",
      "Total Score: 340.0\n",
      "Total Score: 210.0\n",
      "Total Score: 260.0\n",
      "Total Score: 230.0\n",
      "Total Score: 210.0\n",
      "Total Score: 90.0\n",
      "Total Score: 150.0\n",
      "Total Score: 170.0\n",
      "Total Score: 110.0\n",
      "Total Score: 150.0\n",
      "Total Score: 150.0\n",
      "Total Score: 210.0\n",
      "Total Score: 290.0\n",
      "Total Score: 220.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-034a44e5b588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mrandom_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mALL_SCORES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "\n",
    "EPISODES = 500\n",
    "ALL_SCORES = np.zeros(EPISODES)\n",
    "\n",
    "env = gym.make(\"MsPacman-ram-v0\")\n",
    "env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    env.reset()\n",
    "    \n",
    "    reward, info, done = None, None, None\n",
    "\n",
    "    \n",
    "    total_score = 0\n",
    "    while done != True:\n",
    "        env.render()\n",
    "        random_action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(random_action)\n",
    "        total_score += reward\n",
    "    ALL_SCORES[episode] = total_score\n",
    "    print(\"Total Score: {}\".format(total_score))\n",
    "    # print(state, reward, done, info)\n",
    "    \n",
    "env.close()\n",
    "plt.plot(ALL_SCORES)\n",
    "plt.title(\"Random Agent: {} Episodes\".format(EPISODES))\n",
    "plt.show()\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print('Average Score for {} Episodes: {}'.format(EPISODES, np.mean(ALL_SCORES)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2- corrected reward (0.01 LR, batch size 256, min epsilon 0.05, episilon decay 0.9999)  \n",
    "v2- corrected reward (0.05 LR, batch size 256, min epsilon 0.0, episilon decay 0.99999), increased memeory from 2000 to 10000\n",
    "\n",
    "BOTH STILL GET STUCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 25,353\n",
      "Trainable params: 25,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: -3130.0   memory length: 664   epsilon: 0.9933819631079267\n",
      "episode: 1   score: -3100.0   memory length: 1318   epsilon: 0.9869064108282789\n",
      "episode: 2   score: -3005.0   memory length: 1979   epsilon: 0.9804044396314596\n",
      "episode: 3   score: -3165.0   memory length: 2670   epsilon: 0.9736531637189741\n",
      "episode: 4   score: -2825.0   memory length: 3295   epsilon: 0.9675867783156002\n",
      "episode: 5   score: -2610.0   memory length: 3857   epsilon: 0.9621641653234502\n",
      "episode: 6   score: -3515.0   memory length: 4606   epsilon: 0.9549844414578116\n",
      "episode: 7   score: -3120.0   memory length: 5262   epsilon: 0.9487402157537278\n",
      "episode: 8   score: -4150.0   memory length: 6236   epsilon: 0.9395442968972993\n",
      "episode: 9   score: -2665.0   memory length: 6789   epsilon: 0.9343629307104079\n",
      "episode: 10   score: -3090.0   memory length: 7421   epsilon: 0.9284763487481552\n",
      "episode: 11   score: -2330.0   memory length: 7917   epsilon: 0.9238824852884858\n",
      "episode: 12   score: -2690.0   memory length: 8517   epsilon: 0.9183557595007187\n",
      "episode: 13   score: -3130.0   memory length: 9187   epsilon: 0.9122233118898601\n",
      "episode: 14   score: -2285.0   memory length: 9696   epsilon: 0.9075918690963882\n",
      "episode: 15   score: -3065.0   memory length: 10000   epsilon: 0.9015944085678487\n",
      "episode: 16   score: -2885.0   memory length: 10000   epsilon: 0.8960665907788155\n",
      "episode: 17   score: -3580.0   memory length: 10000   epsilon: 0.8892466931078791\n",
      "episode: 18   score: -3160.0   memory length: 10000   epsilon: 0.8830966556216782\n",
      "episode: 19   score: -3300.0   memory length: 10000   epsilon: 0.8769365338119638\n",
      "episode: 20   score: -3610.0   memory length: 10000   epsilon: 0.8702970449733118\n",
      "episode: 21   score: -3600.0   memory length: 10000   epsilon: 0.8636732774418402\n",
      "episode: 22   score: -3495.0   memory length: 10000   epsilon: 0.8558751398383687\n",
      "episode: 23   score: -2405.0   memory length: 10000   epsilon: 0.8514786914576347\n",
      "episode: 24   score: -3485.0   memory length: 10000   epsilon: 0.8452432352709619\n",
      "episode: 25   score: -2495.0   memory length: 10000   epsilon: 0.8408004981021763\n",
      "episode: 26   score: -4715.0   memory length: 10000   epsilon: 0.8322928492210642\n",
      "episode: 27   score: -3490.0   memory length: 10000   epsilon: 0.8260078880945334\n",
      "episode: 28   score: -3120.0   memory length: 10000   epsilon: 0.8205577484908956\n",
      "episode: 29   score: -2620.0   memory length: 10000   epsilon: 0.8159264873110765\n",
      "episode: 30   score: -3000.0   memory length: 10000   epsilon: 0.8106563510055694\n",
      "episode: 31   score: -3890.0   memory length: 10000   epsilon: 0.8035216948316656\n",
      "episode: 32   score: -2910.0   memory length: 10000   epsilon: 0.7984913652039056\n",
      "episode: 33   score: -5160.0   memory length: 10000   epsilon: 0.7898034358529287\n",
      "episode: 34   score: -3165.0   memory length: 10000   epsilon: 0.7845058785174952\n",
      "episode: 35   score: -2950.0   memory length: 10000   epsilon: 0.7795322292318363\n",
      "episode: 36   score: -2505.0   memory length: 10000   epsilon: 0.7754658980661525\n",
      "episode: 37   score: -2375.0   memory length: 10000   epsilon: 0.7713590668997585\n",
      "episode: 38   score: -2215.0   memory length: 10000   epsilon: 0.7677344901507025\n",
      "episode: 39   score: -3070.0   memory length: 10000   epsilon: 0.7626993557888803\n",
      "episode: 40   score: -3295.0   memory length: 10000   epsilon: 0.7572957688664254\n",
      "episode: 41   score: -2905.0   memory length: 10000   epsilon: 0.7525623554865796\n",
      "episode: 42   score: -3280.0   memory length: 10000   epsilon: 0.7472081706834858\n",
      "episode: 43   score: -4125.0   memory length: 10000   epsilon: 0.7396031590310028\n",
      "episode: 44   score: -3070.0   memory length: 10000   epsilon: 0.7346349705013364\n",
      "episode: 45   score: -2640.0   memory length: 10000   epsilon: 0.7304156153829244\n",
      "episode: 46   score: -3305.0   memory length: 10000   epsilon: 0.725400324235082\n",
      "episode: 47   score: -4315.0   memory length: 10000   epsilon: 0.7187212760616173\n",
      "episode: 48   score: -3120.0   memory length: 10000   epsilon: 0.7139361939798144\n",
      "episode: 49   score: -4385.0   memory length: 10000   epsilon: 0.7069666863973751\n",
      "episode: 50   score: -3225.0   memory length: 10000   epsilon: 0.7000792172720033\n",
      "episode: 51   score: -4505.0   memory length: 10000   epsilon: 0.69287072960393\n",
      "episode: 52   score: -2505.0   memory length: 10000   epsilon: 0.6891048366777444\n",
      "episode: 53   score: -3275.0   memory length: 10000   epsilon: 0.6843047656616053\n",
      "episode: 54   score: -3070.0   memory length: 10000   epsilon: 0.6797624160247838\n",
      "episode: 55   score: -3985.0   memory length: 10000   epsilon: 0.6739077995741034\n",
      "episode: 56   score: -3190.0   memory length: 10000   epsilon: 0.6692604327987924\n",
      "episode: 57   score: -4400.0   memory length: 10000   epsilon: 0.662746936114602\n",
      "episode: 58   score: -2970.0   memory length: 10000   epsilon: 0.6583608524412863\n",
      "episode: 59   score: -2990.0   memory length: 10000   epsilon: 0.6538860853539707\n",
      "episode: 60   score: -2700.0   memory length: 10000   epsilon: 0.6501175066221478\n",
      "episode: 61   score: -3160.0   memory length: 10000   epsilon: 0.6454405390095491\n",
      "episode: 62   score: -3435.0   memory length: 10000   epsilon: 0.6405986003188822\n",
      "episode: 63   score: -3270.0   memory length: 10000   epsilon: 0.6341864535152109\n",
      "episode: 64   score: -2055.0   memory length: 10000   epsilon: 0.631256942638876\n",
      "episode: 65   score: -3775.0   memory length: 10000   epsilon: 0.625532278704283\n",
      "episode: 66   score: -2505.0   memory length: 10000   epsilon: 0.6220826150658685\n",
      "episode: 67   score: -2305.0   memory length: 10000   epsilon: 0.6190604226312164\n",
      "episode: 68   score: -2625.0   memory length: 10000   epsilon: 0.6155479537148659\n",
      "episode: 69   score: -3760.0   memory length: 10000   epsilon: 0.6100816545749891\n",
      "episode: 70   score: -3610.0   memory length: 10000   epsilon: 0.6043616403537212\n",
      "episode: 71   score: -2700.0   memory length: 10000   epsilon: 0.600770339727339\n",
      "episode: 72   score: -3125.0   memory length: 10000   epsilon: 0.5965855769139137\n",
      "episode: 73   score: -2900.0   memory length: 10000   epsilon: 0.5929100288126834\n",
      "episode: 74   score: -2270.0   memory length: 10000   epsilon: 0.5900118614893715\n",
      "episode: 75   score: -4295.0   memory length: 10000   epsilon: 0.5841936914995504\n",
      "episode: 76   score: -2780.0   memory length: 10000   epsilon: 0.5806525521218755\n",
      "episode: 77   score: -2605.0   memory length: 10000   epsilon: 0.5773464574019264\n",
      "episode: 78   score: -5165.0   memory length: 10000   epsilon: 0.5702486367628207\n",
      "episode: 79   score: -2350.0   memory length: 10000   epsilon: 0.5672569889067325\n",
      "episode: 80   score: -2950.0   memory length: 10000   epsilon: 0.5636493917515745\n",
      "episode: 81   score: -2855.0   memory length: 10000   epsilon: 0.5601487543458225\n",
      "episode: 82   score: -2930.0   memory length: 10000   epsilon: 0.5565752319198224\n",
      "episode: 83   score: -2275.0   memory length: 10000   epsilon: 0.5538380550255333\n",
      "episode: 84   score: -3035.0   memory length: 10000   epsilon: 0.5500792124622771\n",
      "episode: 85   score: -1970.0   memory length: 10000   epsilon: 0.5477189382736214\n",
      "episode: 86   score: -2125.0   memory length: 10000   epsilon: 0.545221561092152\n",
      "episode: 87   score: -3535.0   memory length: 10000   epsilon: 0.5409042122952031\n",
      "episode: 88   score: -4410.0   memory length: 10000   epsilon: 0.5354578395633177\n",
      "episode: 89   score: -2685.0   memory length: 10000   epsilon: 0.5322387280705241\n",
      "episode: 90   score: -3925.0   memory length: 10000   epsilon: 0.5272854637929131\n",
      "episode: 91   score: -3325.0   memory length: 10000   epsilon: 0.5230159905502952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 92   score: -2560.0   memory length: 10000   epsilon: 0.5201472844321151\n",
      "episode: 93   score: -2275.0   memory length: 10000   epsilon: 0.5173822609146081\n",
      "episode: 94   score: -2285.0   memory length: 10000   epsilon: 0.5148378308198561\n",
      "episode: 95   score: -1935.0   memory length: 10000   epsilon: 0.5126544021940804\n",
      "episode: 96   score: -2830.0   memory length: 10000   epsilon: 0.5095265226873144\n",
      "episode: 97   score: -2455.0   memory length: 10000   epsilon: 0.5066760676449954\n",
      "episode: 98   score: -2405.0   memory length: 10000   epsilon: 0.5039322590607098\n",
      "episode: 99   score: -2590.0   memory length: 10000   epsilon: 0.5010178972056814\n",
      "episode: 100   score: -2665.0   memory length: 10000   epsilon: 0.49804567698007357\n",
      "episode: 101   score: -3485.0   memory length: 10000   epsilon: 0.49365739757560856\n",
      "episode: 102   score: -3305.0   memory length: 10000   epsilon: 0.4901501210934748\n",
      "episode: 103   score: -3470.0   memory length: 10000   epsilon: 0.48648772801487655\n",
      "episode: 104   score: -3305.0   memory length: 10000   epsilon: 0.482934792620067\n",
      "episode: 105   score: -5870.0   memory length: 10000   epsilon: 0.476763983678416\n",
      "episode: 106   score: -2630.0   memory length: 10000   epsilon: 0.4739688233035284\n",
      "episode: 107   score: -4020.0   memory length: 10000   epsilon: 0.4691682729387346\n",
      "episode: 108   score: -3420.0   memory length: 10000   epsilon: 0.4652530509277074\n",
      "episode: 109   score: -2950.0   memory length: 10000   epsilon: 0.4622202090134835\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "# DQN Agent for the MsPacman\n",
    "# it uses Neural Network to approximate q function and replay memory & target q network\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see MsPacman learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.05\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99999\n",
    "        self.epsilon_min = 0.0\n",
    "        self.batch_size = 256\n",
    "        self.train_start = 1000\n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./Saved Weights/pacman.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "#         total_training_steps += batch_size\n",
    "\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] #STATE\n",
    "            action.append(mini_batch[i][1])    #ACTION\n",
    "            reward.append(mini_batch[i][2])    #REWARD\n",
    "            update_target[i] = mini_batch[i][3]#NEXT STATE\n",
    "            done.append(mini_batch[i][4])      #DONE\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "#         print(update_input, update_target)\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EPISODES = 1000\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "    \n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done: \n",
    "            dead = False         \n",
    "            while not dead:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                reward = reward-5 if not dead else -100  # if action make Pacman dead, then gives penalty of -100\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "                \n",
    "               \n",
    "                \n",
    "                \n",
    "                # save the sample <s, a, r, s'> to the replay memory\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                \n",
    "                # every time step do the training\n",
    "                agent.train_model()\n",
    "\n",
    "                state = next_state            \n",
    "                score += reward\n",
    "                dead = info['ale.lives'] < lives\n",
    "                lives = info['ale.lives']\n",
    "                \n",
    "\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./pacman.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "    #         # save the model\n",
    "        if e % 2 == 0:\n",
    "            agent.model.save_weights(\"./Saved Weights/pacman_correctedv11.h5\")\n",
    "\n",
    "#     print(\"Total Training Steps: {}\".format(total_training_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINDING\n",
    "\n",
    "The Pacman learns to just stay in the corner after consuming the \"Power Pellets\" since the reward from consuming ghosts outweight the negative reward of dying. Must increase the negative reward from dying.\n",
    "\n",
    "The learned weights will be in 'correctedv1.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Test Trained Weights- No Training Involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 25,353\n",
      "Trainable params: 25,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 920.0\n",
      "episode: 1   score: 550.0\n",
      "episode: 2   score: 640.0\n",
      "episode: 3   score: 440.0\n",
      "episode: 4   score: 930.0\n",
      "episode: 5   score: 500.0\n",
      "episode: 6   score: 1000.0\n",
      "episode: 7   score: 240.0\n",
      "episode: 8   score: 890.0\n",
      "episode: 9   score: 850.0\n",
      "episode: 10   score: 560.0\n",
      "episode: 11   score: 600.0\n",
      "episode: 12   score: 450.0\n",
      "episode: 13   score: 840.0\n",
      "episode: 14   score: 720.0\n",
      "episode: 15   score: 930.0\n",
      "episode: 16   score: 550.0\n",
      "episode: 17   score: 390.0\n",
      "episode: 18   score: 1150.0\n",
      "episode: 19   score: 1030.0\n",
      "episode: 20   score: 440.0\n",
      "episode: 21   score: 230.0\n",
      "episode: 22   score: 250.0\n",
      "episode: 23   score: 1010.0\n",
      "episode: 24   score: 520.0\n",
      "episode: 25   score: 1000.0\n",
      "episode: 26   score: 950.0\n",
      "episode: 27   score: 280.0\n",
      "episode: 28   score: 240.0\n",
      "episode: 29   score: 470.0\n",
      "episode: 30   score: 160.0\n",
      "episode: 31   score: 1100.0\n",
      "episode: 32   score: 470.0\n",
      "episode: 33   score: 700.0\n",
      "episode: 34   score: 450.0\n",
      "episode: 35   score: 520.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e09061fa5ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m# get action for the current state and go one step in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DQN Agent for the MsPacman\n",
    "# it uses Neural Network to approximate q function and replay memory & target q network\n",
    "class TEST_DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see MsPacman learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = True\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # create main model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./Saved Weights/pacman_correctedv10.h5\")\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        return model\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "#         q_value = self.model.predict(state)\n",
    "#         return np.argmax(q_value[0])\n",
    "        \n",
    "        \n",
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EPISODES = 100\n",
    "    ALL_SCORES = np.zeros(EPISODES)\n",
    "\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/MsPacman-ram-experiment-1',force=True)\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = TEST_DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        lives = 3\n",
    "        while not done: \n",
    "            dead = False         \n",
    "            while not dead:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "                state = next_state            \n",
    "                score += reward\n",
    "                dead = info['ale.lives']<lives\n",
    "                lives = info['ale.lives']\n",
    "                # if an action make the Pacman dead, then gives penalty of -100\n",
    "                reward = reward if not dead else -500\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./pacman.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score)\n",
    "        \n",
    "        ALL_SCORES[e] = score\n",
    "                \n",
    "    env.close()\n",
    "    plt.plot(ALL_SCORES)\n",
    "    plt.title(\"Random Agent: {} Episodes\".format(EPISODES))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "    print('Average Score for {} Episodes: {}'.format(EPISODES, np.mean(ALL_SCORES)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
